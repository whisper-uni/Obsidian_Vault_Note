{"IntroduzioBigData/Big-Data/Big-Data":{"slug":"IntroduzioBigData/Big-Data/Big-Data","filePath":"IntroduzioBigData/Big Data/Big Data.md","title":"Big Data","links":["Social-Analitycs","IntroduzioBigData/Machine-Learning/Machine-Learning"],"tags":[],"content":"\nIl Big Data è definito come:\nEstrazione “conosciuta” da un immenso volume, varietà e velocità dei dati, nel contesto in cui si trovano, oltre ciò che\nera precedentemente possibile.\nCi si riferisce ad una estrazione “intuitiva” di informazioni che possono apportare un certo valore. Tutto questo deriva da una necessità di governare la cosiddetta “Digital Density”, cioè il valore che i dati portano all’economia, attraverso l’integrazione delle interazioni tra i singoli Data Points, oltre alla alla necessità di governare questa moltitudine di informazioni.\nCaratteristiche fondamentali dei Big Data sono le cosiddette “5 + 1 V” dei Big Data:\nVolume\nLe organizzazioni devono gestire più dati in diverse forme attraverso un ampio numero di sistemi, considerando che il costo dello storage (per costo di realizzazione) è in diminuzione, al contrario del processing power1\nVarietà\nI dati non sono omogenei, bensì molto eterogenei: il 20% è dato strutturato, l’80% non strutturato\nVelocità\nIndica il rate a cui i dati sono generati e consumati, o l’abilità di fare streaming e/o processare le informazioni che fluiscono a velocità rapida, spessissimo in real time.\nVeridicita\nÈ necessario conoscere la provenienza del dato e l’integrità, tenere traccia della storia di eventuali cambiamenti apportati ai dati (la cosiddetta retention) quanto tempo vanno mantenuti i dati e con quali modalità, e fare una analisi approfondita circa la rilevanza di essi in alcune attività.\nVulnerabilità\nI dati devono essere protetti e la sicurezza deve essere considerata durante la progettazione dei sistemi; importante è la sicurezza multi-livello.\nValore\nLe organizzazioni che orientano il loro Business su attività di Data Driven, e che quindi adottano sistemi per l’acquisizione, l’analisi e la gestione dei dati, sono nella media il 5% più profittevoli dei loro competitors, riuscendo a portare valore a loro stesse e nell’ambito in cui si trovano.\ndata life-cycle\nAspetto fondamentale è legato al Valore del dato che diminuisce nel tempo. Si parla di “dimezzamento” del dato: il rate a cui questi dati decadono è diverso a seconda dell’ambito; stessa discorso vale per i fattori che impattano questo decadimento, diversi a seconda della categoria (detta data item). In un diagramma in cui sull’asse delle ascisse poniamo l’half-life dei dati, se questo è basso il rate di decadimento del valore del dato è veloce (è il caso di retail, internet, turismo), se è alto il rate di decadimento è più basso (healthcare,chemistry, industry, agricoltura).\nAnalisi\nil tipo di relazione che si ha con la catena di valore del dato stesso.\nIl Valore che otteniamo a fronte dell’utilizzo dei dati è tanto più alto passando da un’analisi descrittiva verso una prescrittiva.\nLa distinzione tra le diverse tipologie di analisi:\nDescrittive\nL’analisi descrittiva è il livello più basilare e risponde alla domanda &quot;Cosa è successo?&quot;. Si concentra sulla sintesi e sulla rappresentazione dei dati storici per capire le caratteristiche di base di un fenomeno. È come scattare una fotografia di ciò che è accaduto.\nCaratteristiche principali:\n\nRiepiloga i dati passati: Utilizza statistiche come media, mediana, moda, deviazione standard per descrivere i trend e le distribuzioni.\nVisualizzazione: Spesso presenta i dati tramite grafici, tabelle e dashboard per renderli facilmente comprensibili.\nObiettivo: Fornire una chiara panoramica di eventi passati, identificare modelli e anomalie.\nEsempi:\n\nUn’azienda che analizza le vendite del trimestre precedente per capire quali prodotti hanno venduto di più.\nUn’analisi del traffico di un sito web per vedere quante visite ha ricevuto e da quali fonti.\nIl calcolo del numero di clienti persi nell’ultimo anno.\nSemplici analisi che permettono di condensare tutti i Big Data in informazioni più piccole e gestibili. Con questa tipologia di analisi è nata la Business Intelligence (BI), il Data Warehouse e il Reporting. Lo scopo è quello di “sommarizzare” ciò che è avvenuto attraverso i dati. Più dell’80% delle Business Analytics - includendo Social Analitycs - sono descrittive.\n\n\n\nPredittive\nL’analisi predittiva va oltre il semplice racconto del passato e cerca di rispondere alla domanda &quot;Cosa potrebbe succedere?&quot;. Utilizza dati storici, modelli statistici e algoritmi di machine learning per prevedere eventi futuri o risultati probabili.\nCaratteristiche principali:\n\nPrevisione: Sviluppa modelli che imparano dai dati passati per fare previsioni su eventi futuri.\nProbabilità: Non fornisce certezze assolute, ma stime di probabilità su ciò che potrebbe accadere.\n==Tecniche: Si avvale di tecniche come regressione, alberi decisionali, reti neurali e altri algoritmi di machine learning.==\nObiettivo: Anticipare le tendenze, identificare i rischi e le opportunità future.\nEsempi:\n\nUn’azienda di e-commerce che predice quali prodotti un cliente potrebbe acquistare in base alla sua cronologia di acquisti.\nPrevisione della domanda di un prodotto per ottimizzare la gestione delle scorte.\nValutazione del rischio di insolvenza di un cliente basata sul suo storico finanziario.\nPrevisione del fallimento di un componente meccanico per la manutenzione preventiva.\nQueste analisi utilizzano una serie di statistiche, tecniche di modellazione, Data Mining e Machine Learning per valutare dati storici e recenti, da fornire in input ai sistemi, al fine di effettuare valutazioni in termini probabilistici di quello che il modello genera come output.\n\n\n\nPrescrittive\nL’analisi prescrittiva è il livello più avanzato e sofisticato, e risponde alla domanda &quot;Cosa dovremmo fare?&quot;. Non solo prevede cosa potrebbe accadere, ma suggerisce le migliori azioni da intraprendere per ottenere un risultato desiderato o evitare un esito negativo. Va oltre la previsione, fornendo raccomandazioni concrete e attuabili.\nCaratteristiche principali:\n\nRaccomandazioni: Fornisce opzioni e suggerimenti su come agire per influenzare i risultati.\n==Ottimizzazione: Spesso utilizza tecniche di ottimizzazione, simulazione e machine learning per trovare la soluzione migliore considerando vari vincoli e obiettivi.==\nDecisione: Aiuta a automatizzare e migliorare il processo decisionale, rendendolo più basato sui dati.\nObiettivo: Ottimizzare i risultati, mitigare i rischi e guidare le azioni per raggiungere obiettivi specifici.\nEsempi:\n\nUn sistema che suggerisce il percorso di consegna più efficiente per una flotta di veicoli, considerando traffico, tempo e costi.\nUn’azienda che consiglia strategie di prezzo dinamiche per massimizzare i ricavi in base alle previsioni di domanda.\nSistemi sanitari che raccomandano piani di trattamento personalizzati per i pazienti, basandosi sulle previsioni di efficacia e sui profili dei pazienti.\nUn sistema di gestione del portafoglio finanziario che suggerisce quali investimenti fare per massimizzare i profitti e minimizzare i rischi.\nTali tecniche vengono adottate qualora sia necessario “prescrivere” un’azione, così che il Decision Maker possa prendere le informazioni risultanti e agire di conseguenza. Utilizzano algoritmi che valutano probabilisticamente quale sia la migliore azione da intraprendere per arrivare ad un certo obiettivo, o raccomandano più possibili azioni, mostrando l’outcome più probabile per ogni decisione. L’idea è quella di generare una raccomandazione rispetto a quella che può essere la migliore ed efficace azione da intraprendere.\n\n\n\nSintesi\nl’analisi descrittiva ci dice cosa è successo, l’analisi predittiva ci dice cosa potrebbe succedere, e l’analisi prescrittiva ci dice cosa dovremmo fare per influenzare attivamente il futuro. Nel contesto dei Big Data, queste tre tipologie di analisi sono fondamentali per trasformare grandi volumi di dati in decisioni strategiche e vantaggi competitivi."},"IntroduzioBigData/Data-Governance/Data-Governance":{"slug":"IntroduzioBigData/Data-Governance/Data-Governance","filePath":"IntroduzioBigData/Data Governance/Data Governance.md","title":"Data Governance","links":["IntroduzioBigData/Big-Data/Big-Data"],"tags":[],"content":"La Data Governance consiste nella pianificazione e nell’esecuzione di pratiche volte ad acquisire, controllare, proteggere e trasmettere dati, oltre ad incrementarne il Valore dei dati stessi e degli asset informativi di una organizzazione, attraverso l’esecuzione delle diverse tipologie di Analisi. Quindi rappresenta l’insieme di tutti quei processi che trasformano i dati elementari in informazioni che portano valore al business di un’azienda, e a tutti coloro che usano i sistemi informativi che li contengono.\nI dati strutturati hanno un ciclo di vita ben definito:\nche parte dall’acquisizione\nprocede con una graduale pulizia (cleaning);\nseguono politiche di discovery volte ad effettuare una prima valutazione.\nVengono quindi normalizzati affinché assumano le stesse caratteristiche\npoi aggregati a seconda delle dimensioni di analisi volute\nper poi finire con una ottimizzazione che faciliti l’accesso agli utenti.\nI dati non strutturati hanno un ciclo di vita comunque ben definito:\nsi parte dall’acquisizione,\na cui segue un passo in funzione della tipologia di dato (Audio Split e Audio Deciphering, Video Frame identification e Image Recognition, Text Correction e Text Tagging).\nIl passo successivo, nel caso di testo, è quello di indicizzare il testo, estrarre delle entità e fare un’analisi semantica per poi comprendere il significato, e poi eventualmente fare Sentiment Analysis; nel caso di audio e/o video si procede ad inferire i metadati relativi ai contenuti.\nData governance principles\nObiettivo della Data Governance è quello di fornire valore ad una organizzazione attraverso il processamento dei dati con una certa velocità, pur sempre mantenendo alta la qualità con l’utilizzo di strumenti pratici, oltre a considerare la sicurezza. Tutte le politiche e i processi devono considerare la dimensione del valore che si vuole portare alle organizzazioni, oltre alla costo/efficacia delle procedure e alla praticità dei sistemi informativi, che hanno un impatto sui costi.\nDi seguito solo elencati e dettagliati quelli che sono i principi della Data Governance.\ndata retention\nL’informazione deve essere mantenuta quanto fisicamente possibile, con i vincoli dati dalle leggi governative, dalle etiche delle aziende e dalla privacy. D’altra parte è importante considerare che l’informazione ha un suo ciclo di vita (data life-cycle) e il suo valore (Valore)decade nel tempo, quindi si deve mantenere l’informazione su storage il cui costo è relazionato al valore del dato.\ndata quality\nI decisori devono poter accedere ai dati ed essere in grado di comprendere il timing con cui questi arrivano, le procedure di riconciliazione dei dati, la completezza e l’accuratezza con cui sono forniti. La qualità del processo di trasformazione dei dati deve essere definita ad ogni passo, dall’acquisizione fino alla trasmissione; devono essere robusti anche gli algoritmi usati per la valutazione della qualità.\ndata access\nQualsiasi persona all’interno dell’azienda deve poter accedere alle informazioni, a patto che non vi siano specifiche ragioni legali, commerciali o etiche per cui non devono essere disponibili ad un individuo. Oramai le informazioni sono disseminate all’interno di una organizzazione, motivo che ne fa crescere il valore; quindi sono necessarie alcune limitazioni all’accesso.\ndata custody\nOgni data item deve essere gestito da una singola persona con un ruolo specifico. Deve essere creata una matrice di responsabilità in modo da garantire che potenziali problemi siano riconducibili ad uno specifico individuo: da un lato vi è la dimensione tecnica (data stuart responsabile dei processi dei dati), dall’altra vi è la dimensione di business (data owner), responsabile del dato pubblicato verso gli utenti finali.\ndata compliance\nIn qualsiasi sistema informativo i dati contenuti sono soggetti a regole da parte di governi locali, regionali, nazionali o internazionali; questi vincoli devono essere messi in pratica nelle politiche di Data Governance. Nel processo quindi deve essere garantita la conoscenza aggiornata di tutte le leggi dell’ambito applicativo di pertinenza, e queste devono mettere in pratica.\ndata mapping\nLa rilevazione e la registrazione di dati che variano velocemente nel tempo attraverso l’intera organizzazione, permette di monitorare e migliorare l’efficacia di business, tenendo conto di quale sia stata la sorgente dei dati, e considerando il mapping interno all’organizzazione che ne ha permesso la pubblicazione.\ndata meaning\nTutti gli stakeholders devono poter conoscere il significato dei dati pubblicati e avere una chiara visione di essi: questo è possibile grazie ad una riduzione della ridondanza, dell’ambiguità e dell’inconsistenza, che amplificano la qualità dell’informazione.\ndata 3rd -party-control\nBisogna controllare i dati che vengono scambiati con terze parti che giocano sempre più un ruolo importante, sia perché vengono forniti dati alle aziende esterne, sia perché queste terze parti sono sottoscrittori di dati, o ancora perché si è deciso di dare la delivery dei dati in outsourcing. Si ha quindi la necessità di avere interscambio di dati, quindi è necessario organizzarli per il business. D’ora in avanti per la descrizione degli aspetti della modellazione ci si soffermerà sul meaning e sul\nmapping.\nInformation systems modelling concepts\nVi sono diverse tecniche di modellazione dei dati che possono essere molto utili nell’ambito della progettazione di soluzioni Big Data. Vediamole nel seguito.\nContext Diagram (CD)\nRappresenta in termini generali l’interazione del sistema con il mondo esterno: con altri sistemi o con gli utenti che interagiscono con esso. E’ utile quindi per rappresentare in termini generali come avviene l’interazione con il sistema, oltre a chiarificare le interfacce, e a rappresentarne i limiti.\nEntity Relationship Diagram (ERD)\nE’ una rappresentazione grafica che rappresenta le relazioni tra persone, oggetti, concetti o eventi all’interno del sistema informativo. E’ molto utilizzato per la rappresentazione delle basi di dati, pur avendo il limite di essere molto legato a questi; è una modellazione logica su cui basare quella fisica.\nData Flow Diagram (DFD)\nE’ una rappresentazione dei dati e del flusso dei dati attraverso il sistema informativo, oltre che dei processi di trasformazione dei Big Data. Modella gli aspetti del processo in maniera indipendente dalla tecnologia.\nDimensional fact modelling (DFM)\nLa Dimensional Fact Modeling (DFM) è una rappresentazione di fatti, misure, dimensioni e gerarchie utilizzate all’interno di un sistema, che permettono agli utenti di accederlo. A livello concettuale è indipendente dalle tecnologie, e permette facilmente di modellare sistemi analitici anche in ambito Big Data.\nè un formalismo grafico ad hoc appositamente ideato per supportare la fase di modellazione concettuale in un progetto di data warehouse. DFM può essere utilizzato anche da analisti e utenti non tecnici.\nIl DFM permette di comprendere quindi quali sono i dati disponibili ai fini del consumo dei dati stessi, e come essi possano essere acceduti secondo le varie dimensioni di analisi. Abilita la comprensione dell’informazione, ma non permette di costruire un glossario delle informazioni, quindi un metodo di condivisione più semplice tra tutti coloro che interagiscono con il sistema e che hanno bisogno di dati pubblicati. Il DFM punta su due aspetti chiave della Data Governance:\n•meaning, glossario condiviso dagli utenti;\n•mapping, consente di avere una visione singola dei dati pubblicati.\nfatto\nRappresenta qualcosa di reale e concreto, oggetto di analisi (modella un evento che avviene nella realtà) circa l’ambito di interesse per il processo di analisi dei dati. Ad esempio: vendita, spedizione,assunzione.\nmisure\nSono attributi numerici continui che esistono all’interno di un fatto, e lo descrivono da diversi punti di vista. Ad esempio: ogni vendita è misurata dai suoi ricavi.\ndimensione\nSono attributi discreti che determinano la minima granularità adottata per rappresentare i fatti. Ad esempio tipiche per la vendita sono il prodotto, il negozio e la data.\ngerarchia\nSono dimensioni discrete legate da relazioni uno-a-uno, e determinano come i fatti possano essere aggregati e selezionati, ai fini del processo di analisi. Esistono le gerarchie conformi, che sono comuni a più fatti all’interno dello schema informativo che si stia osservando. Ad esempio: gerarchie di tempo o geografiche."},"IntroduzioBigData/Excalidraw/Drawing-2025-06-04-23.08.52.excalidraw":{"slug":"IntroduzioBigData/Excalidraw/Drawing-2025-06-04-23.08.52.excalidraw","filePath":"IntroduzioBigData/Excalidraw/Drawing 2025-06-04 23.08.52.excalidraw.md","title":"Drawing 2025-06-04 23.08.52.excalidraw","links":[],"tags":["excalidraw"],"content":"⚠  Switch to EXCALIDRAW VIEW in the MORE OPTIONS menu of this document. ⚠ You can decompress Drawing data with the command palette: ‘Decompress current Excalidraw file’. For more info check in plugin settings under ‘Saving’\nExcalidraw Data\nText Elements\ncome si fa \nj "},"IntroduzioBigData/Excalidraw/Drawing-2025-06-09-22.48.04.excalidraw":{"slug":"IntroduzioBigData/Excalidraw/Drawing-2025-06-09-22.48.04.excalidraw","filePath":"IntroduzioBigData/Excalidraw/Drawing 2025-06-09 22.48.04.excalidraw.md","title":"Drawing 2025-06-09 22.48.04.excalidraw","links":[],"tags":["excalidraw"],"content":"⚠  Switch to EXCALIDRAW VIEW in the MORE OPTIONS menu of this document. ⚠ You can decompress Drawing data with the command palette: ‘Decompress current Excalidraw file’. For more info check in plugin settings under ‘Saving’\nDrawing\nN4IgLgngDgpiBcIYA8DGBDANgSwCYCd0B3EAGhADcZ8BnbAewDsEAmcm+gV31TkQAswYKDXgB6MQHNsYfpwBGAOlT0AtmIBeNCtlQbs6RmPry6uA4wC0KDDgLFLUTJ2lH8MTDHQ0YNMWHRJMRZFAEYQljIkT1UYRjAaBABtAF1ydCgoAGUAsD5QSXw8LOwNPkZOTExyHRgiACF0VABrQq5GXABhekx6fAQQAGIAM1GxkABfCaA==\n%%"},"IntroduzioBigData/Hadoop-and-MapReduce/Drawing-2025-06-15-12.39.51.excalidraw":{"slug":"IntroduzioBigData/Hadoop-and-MapReduce/Drawing-2025-06-15-12.39.51.excalidraw","filePath":"IntroduzioBigData/Hadoop&MapReduce/Drawing 2025-06-15 12.39.51.excalidraw.md","title":"Drawing 2025-06-15 12.39.51.excalidraw","links":[],"tags":["excalidraw"],"content":"⚠  Switch to EXCALIDRAW VIEW in the MORE OPTIONS menu of this document. ⚠ You can decompress Drawing data with the command palette: ‘Decompress current Excalidraw file’. For more info check in plugin settings under ‘Saving’\nDrawing\nN4IgLgngDgpiBcIYA8DGBDANgSwCYCd0B3EAGhADcZ8BnbAewDsEAmcm+gV31TkQAswYKDXgB6MQHNsYfpwBGAOlT0AtmIBeNCtlQbs6RmPry6uA4wC0KDDgLFLUTJ2lH8MTDHQ0YNMWHRJMRZFAEYQljIkT1UYRjAaBABtAF1ydCgoAGUAsD5QSXw8LOwNPkZOTExyHRgiACF0VABrQq5GXABhekx6fAQQAGIAM1GxkABfCaA==\n%%"},"IntroduzioBigData/Hadoop-and-MapReduce/HDFS":{"slug":"IntroduzioBigData/Hadoop-and-MapReduce/HDFS","filePath":"IntroduzioBigData/Hadoop&MapReduce/HDFS.md","title":"HDFS","links":["IntroduzioBigData/Big-Data/Big-Data","IntroduzioBigData/Hadoop-and-MapReduce/Hadoop","IntroduzioBigData/Hadoop-and-MapReduce/HDFS"],"tags":[],"content":"HDFS è un file system distribuito progettato per memorizzare file di grandi dimensioni, con modalità di accesso in streaming, e che gira su un cluster di commodity hardware. E’ nato per gestire una modalità di lettura e scrittura di file batch, effettuando operazioni su questo per ottenere una serie di informazioni descrittive dei file di grande volume.\nHDFS gestisce i Big Data  su un cluster di macchine con modello di accesso ai dati in streaming. Utilizza l’archiviazione distribuita per fornire una visualizzazione del disco singolo e per fornire uno spazio dei nomi globale univoco sull’archiviazione (^nameNode) distribuita.\nSi tratta di un file system appositamente progettato con funzionalità quali:\ndistribuzione\ntolleranza agli errori\nreplica\ndistribuito su macchine a basso costo e inaffidabili.\nDal punto di vista dell’utente, sembra essere un grande spazio di archiviazione centralizzato, ma dal punto di vista del sistema è il singolo server che contribuisce al suo spazio di archiviazione. HDFS fornisce l’astrazione del file, il che significa che un file oltre la dimensione del disco di archiviazione viene partizionato in HDFS Block e archiviato in un cluster di nodi. Per un utente normale, il file enorme viene logicamente mostrato come un singolo file, ma in realtà parti di questo file sono archiviate in diversi nodi nel cluster. HDFS è immutabile, il che significa che è possibile solo il caricamento iniziale, non esiste alcuna funzionalità di modifica dei file in HDFS utilizzando vi, gedit, ecc. poiché comporta un enorme sovraccarico di I/O, non supporta l’operazione di aggiornamento, pertanto i dati in HDFS vengono scritti una volta e letti molte volte (WORM)\n\n\n                  \n                  Quando uso l&#039;HDFS? \n                  \n                \n\n\nUsa HDFS quando vuoi\n• archiviare dati di grandi dimensioni (oltre TB) su server di base.\n• elaborare un numero limitato di file grandi rispetto a un numero elevato di file piccoli files.\n• letture batch invece di letture/scritture casuali.\n\n\n\n\n\n                  \n                  Quando non uso l&#039;HDFS \n                  \n                \n\n\nHDFS ha tre sottocomponenti:\none ore more Name Nodes (masters)\na set of data nodes (workers) Name nodes maintain the filesystem namespace Datanodes store and retrieve blocks\nflowchart LR\nA[HDFS] --&gt; B(Name Node) \nA --&gt; D(Secondary Name Node) \nA --&gt; C(Data Node)\nB--&gt;E(HDFS Block)\nC--&gt; E\nE--&gt;F(128 MB)\n\n\nName Node (NN) : NN è un servizio/daemon centralizzato che funge da gestore di archiviazione del cluster. ^NameNode\nLe sue responsabilità principali sono:\n• mantenimento dei metadati di file e directory,  il namespace del filesystem e tutti i metadati accessori che lo definiscono (ultima modifica, ultimo accesso, privilegi)\n• controllo e coordinamento dei DN per le operazioni del sistema file come creare, leggere, scrivere, ecc. I client comunicano con NN per eseguire le operazioni quotidiane. A sua volta, NN fornisce ai client la posizione dei DN nel cluster (dove è disponibile il blocco dati) per eseguire operazioni.\nSecondary Name Node (SNN) : è La replica dei Name Node garantisce una prevenzione alla perdita dei dati, quindi alta affidabilità, pur essendo comunque Single Point of Failure (SPoF). Hadoop ha introdotto il concetto di active/stand-by: in caso di failure del Name Node attivo, un Name Node precedentemente  (SNN)mpassivo viene attivato per gestire il tutto, spostando i dati nell’area di memoria del Name Node che era passivo. Questo però non garantisce l’assenza di perdita dei dati, poiché non vi è una replica costante.\nData Node (DN) : Il demone DN è responsabile della gestione dei dischi di archiviazione locali. Gestisce le operazioni del file system come la creazione, la lettura, l’apertura, la chiusura e l’eliminazione dei blocchi.\nIl DN non sa nulla dei blocchi dati. Memorizza ogni blocco come file nel file system locale. I blocchi vengono caricati/eliminati nei DN in base alle istruzioni di NN, che convalida ed elabora le richieste dei client. **NN non esegue alcuna operazione di lettura/scrittura per i client. I clienti comunicano con NN per conoscere la posizione dei blocchi e reindirizzati ai DN per eseguire operazioni di lettura/scrittura. \n\nOgni blocco può essere in cache ad un solo Data Node, cosicché il ==Name Node sappia in quale Data Node è cachato il blocco, quindi darà un accesso privilegiato alle operazioni dei Map su quello specifico nodo.\nHDFS Block\nL’unità di archiviazione e accesso ai dati in HDFS è un blocco, che denota la quantità minima di dati che possono essere archiviati e recuperati da HDFS. I dati che volevamo caricare su HDFS vengono divisi in blocchi di uguali dimensioni e archiviati nei DN nel cluster. I DN memorizzano ciascun blocco di dati come file nel relativo file system locale. Il file system Linux utilizza una dimensione di blocco logico di 4 KB (un gruppo di più blocchi fisici) . I blocchi piccoli sono adatti per i database transazionali.. FS di Linux mantiene i metadati a livello file. HDFS utilizza la dimensione del blocco logico di 128 MB. Pertanto, mantiene i metadati a livello di blocco semplificando la gestione di tali blocchi da parte del Name Node. HDFS è un file system logico sopra file system locale. Non è un vero e proprio sistema file come ext3, ext4 che funziona a livello di disco fisico. Pertanto, HDFS viene eseguito su sistemi file locali (ext3/ext4) e non interagisce direttamente con i dispositivi di archiviazione\nUsing Filesystem APIs\nIl file system è una classe astratta che rappresenta un generico file system, il cui accesso è governato da Application Programming Interface (API). Per creare una istanza di un generico file system, si chiama il metodo FileSystem.get(); quindi la classe HdfsWrite chiama il metodo create() per creare un file su HDFS. Per leggere i dati da un file, si usa la classe HdfsReader, che chiama il metodo open() per aprire un file in HDFS, che ritorna un InputStream che può essere utilizzato per leggere i contenuti di un file. Per creare delle directory, il file system fornisce il metodo mkdirs(Path f), dato un certo path (f); se il path non esiste viene creato. Su un file system possono essere fatte qualsiasi operazioni, come ad esempio listStatus (per avere una lista di file), getFileStatus (per avere informazioni sul file), globalStatus (per gestire file patterns), e delete (per cancellare).\n\n\n                  \n                  Esempio : Read file from the local file system and write it to HDFS \n                  \n                \n\n\nConfiguration conf = getConf();\nOutputStream os = fs.create(outputPath);\nInputStream is = new BufferedInputStream(new FileInputStream(localInputPath));\nIOUtils.copyBytes(is, os, conf);\n\n\n\nLettura di un File\nVediamo come avviene un’operazione di lettura di un file, e come viene distribuita tra Data Node e Name Node.\nPer la lettura di un file:\n\nsi apre una connessione al DistributedFileSystem, il quale legge dal Name Node le invocazioni dei blocchi, che vengono restituiti ordinati rispetto alla facilità di accesso ai nodi che contengono i file, dando priorità ai Data Node che sono sullo stesso rack, piuttosto che a quelli presenti sullo stesso nodo in cui gira il client; (RACK AWARENESS)\nsi leggono i vari Data Node per poi, alla fine, chiudere la connessione con il file system tramite la funzione FSDataInputStream.\n\n\nPassaggio 1: l’utente avvia un comando di lettura al client HDFS, che inoltra la richiesta a NN per trovare la posizione dei blocchi per il file richiesto.\nPassaggio 2: NN fa riferimento ai suoi metadati e trova un elenco di DN in cui è stato archiviato ciascun blocco. Questo elenco viene preparato in base alla posizione (riconoscimento nel rack) delle copie in blocco.\nPassaggio 3: il client HDFS è pronto per leggere blocchi da diversi DN creando un flusso di input HDFS.\nPassaggio 4: il client HDFS preferisce i DN vicini al client (per ridurre il traffico di rete). Se il primo DN dell’elenco non è raggiungibile, viene scelto il secondo DN dell’elenco.\nPasso 5: I blocchi di un file vengono letti in sequenza uno per uno secondo l’ordine di costruzione del file originale. Non è utile leggere più blocchi contemporaneamente. Infine, i DN trasmettono i blocchi di dati al client in ordine\nScrittura di un file\nUn HDFS Client che gira su un Client Node crea una istanza della classe DistributedFileSystem (1), la quale a sua volta tramite una Remote Procedure Call (RPC) (2) crea (Create) in un NameNode una istanza del file. Il NameNode a questo punto verifica che il Client abbia tutti i privilegi necessari per scrivere quel file nella posizione in cui è richiesto. Quando questo è verificato, se possibile (altrimenti viene generata un Exception),\nl’HDFS Client crea una istanza della classe FSDataOutputStream e a questo punto può esser cominciata la scrittura (3) vera e propria.\nLa classe FSDataOutputStream splitta la scrittura (4) in tanti pacchetti di dati, che vengono distribuiti iniziando dal DataNode identificato dal NameNode, e man mano che i pacchetti vengono scritti sono generate delle repliche su ulteriori DataNode. Oltre questo viene generata una coda di Acknowledge (5) per la verifica dell’effettiva scrittura del pacchetto.\nUna volta che il file viene ad essere chiuso l’HDFS Client manda una richiesta di chiusura (6) al FSDataOutputStream: questo rilegge la coda di Acknowledge per verificare che sia completata, e permette la chiusura del file avvisando anche il NameNode, che avrà chiaro tutta la disposizione del file sui vari DataNode.\n\nPassaggio 1: una volta che il client HDFS riceve la richiesta di caricamento del file, divide fisicamente il file in blocchi  Blocco e richiede NN dove archiviare tutti questi blocchi.Poiché il client HDFS non sa quale DN dispone di spazio libero e dove archiviarlo in base al riconoscimento del rack(RACK AWARENESS).\nPassaggio 2: NN si riferisce ai metadati (file FSImage) e determina tre posizioni in base alla consapevolezza del rack per ciascun blocco. Quindi, NN invia un elenco di tre posizioni DN per ciascun blocco. Supponiamo che le posizioni per il blocco 1 siano DN1, DN2 e DN3. Il suggerimento del numero di DN dipende dalla RF. Per impostazione predefinita, vengono suggeriti tre DN per ciascun blocco poiché RF predefinito è 3. Se il nodo client si trova nel cluster Hadoop, è il primo nodo a copiare il blocco dati.\nPassaggio 3: dopo aver ricevuto tre posizioni per ciascun blocco, il client HDFS è pronto per scrivere i blocchi rispettivi DN in parallelo aprendo il flusso di output HDFS. Non vi è alcun vincolo che il blocco 2 debba copiato su HDFS solo dopo il blocco 1. Tutti i blocchi vengono copiati nel cluster in base al numero di thread che un client HDFS può servire.\nPassaggio 4: il client HDFS divide ulteriormente logicamente il blocco 1 in una coda di pacchetti (ciascuno da 64 KB), denominata coda dati. Per block1, il client HDFS forma una pipeline con DN1, DN2 e DN3.\nPassaggio 5: il primo pacchetto dalla coda dati viene copiato su DN1, che memorizza e inoltra il pacchetto a DN2 lungo la pipeline.\nPassaggio 7: il client HDFS riceve una conferma dai DN che indica che il blocco è stato copiato correttamente. Il client HDFS invia un messaggio di successo all’utente. Tutti questi processi sono altamente trasparenti per l’utente. L’utente non sa dove sono archiviati i blocchi di dati."},"IntroduzioBigData/Hadoop-and-MapReduce/Hadoop":{"slug":"IntroduzioBigData/Hadoop-and-MapReduce/Hadoop","filePath":"IntroduzioBigData/Hadoop&MapReduce/Hadoop.md","title":"Hadoop","links":["IntroduzioBigData/Hadoop-and-MapReduce/HDFS","IntroduzioBigData/Hadoop-and-MapReduce/MapReduce"],"tags":[],"content":"Hadoop: una piattaforma scalabile e affidabile per lo storage e l’analisi condivisa, che è Open Source e gira su hardware commodity, quindi è costo-efficace. Se è necessario avere una maggiore capacità di risorse, basterà aggiungerne qualcuna, per rendere più efficace l’intero sistema (scalabilità orizzontale).\nEsistono due strumenti principali in Hadoop, sui quali sono installati tutti gli altri strumenti. •\nFile system distribuito Hadoop: parte di archiviazione (HDFS)\nE’ composto dalle seguenti componenti\n\nNN: gestisce i metadati del file system, controlla e coordina i DN.\nDN: gestisce i dischi locali, archivia e recupera blocchi di dati su istruzioni NN.\nSNN: conserva una copia dei metadati da NN per evitare SPOF.\n\nMapReduce – parte di elaborazione (MapReduce)\nè un modello di programmazione distribuito, scalabile, tollerante ai guasti e parallelo per l’elaborazione di big data su un cluster di macchine di base a basso costo e inaffidabili. MR consente di scrivere lavori distribuiti e scalabili con poco sforzo MR ha due sottocomponenti:\n\nJob Tracker (JT)\n\nJT è un demone in esecuzione in un server dedicato nel cluster per gestire le risorse del cluster e i lavori MR. Le responsabilità principali di JT sono:\n\ngestione e monitoraggio delle risorse (CPU e memoria) nei TT.\npredisposizione del piano esecutivo e coordinamento delle fasi.\npianificazione della mappatura/riduzione delle attività al TT.\ngestione del ciclo di vita dei lavori (dal momento della presentazione fino al completamento).\nmantenere la cronologia dei lavori (statistiche a livello di lavoro).\nfornire tolleranza agli errori.\nJT è inoltre in grado di riconoscere il rack durante la pianificazione delle attività di mappatura/ riduzione. Nella maggior parte dei casi, l’elaborazione avviene nei nodi in cui il blocco dati richiesto è fisicamente disponibile per ridurre il traffico di rete.\n\n\n\n\nTask Tracker (TT).\n\nTT è un demone che viene eseguito in ogni nodo slave nel cluster Hadoop.\nGestisce le risorse:\n\nlocali (CPU e memoria) come uno slot. Uno slot (chiamato anche contenitore) è un pacchetto\nlogico fisso di una porzione di memoria e CPU per eseguire l’attività di mappatura/riduzione\n\n\n\n\n\n\n\n                  \n                  Quando inizia il job di MapReduce \n                  \n                \n\n\nIl job MapReduce può essere avviato solo dopo che tutti i blocchi sono stati caricati correttamente in HDFS. L’utente ha anche il privilegio di specificare la dimensione del blocco, RF durante il caricamento di big data.\n\n\n\n\n\n                  \n                  Quando Utilizzare MapReduce? \n                  \n                \n\n\nLa MR è comunemente preferita per le seguenti applicazioni:\n• Ricerca, ordinamento, raggruppamento.\n• Statistiche semplici: conteggio, classifica.\n• Statistiche complesse: PCA, covarianza.\n• Pre-elaborazione di enormi quantità di dati per applicare algoritmi di apprendimento automatico.\n• Classificazione: bayes naïve, foresta casuale, regressione.\n• Clustering: k Means, gerarchico, densità, bi-clustering.\n• Elaborazione del testo, creazione di indici, creazione e analisi di grafici, riconoscimento di modelli, filtraggio collaborativo, analisi del sentiment\n\n\n\nMAP\n\nI dati vengono suddivisi in vari split (di dimensioni pari ad un blocco HDFS, ovvero 128 Mb), e ciascuno viene inviato su un Data Node di elaborazione diverso;\nA valle dello split ciascuno di questi dati in input verrà posto a processamento da parte di un map task, che genererà l’output relativo secondo l’approccio key-value. I map task devono partizionare l’output (merge) e mantenerlo nel singolo Data Node. Questi output vengono poi distribuiti (shuffle) su vari Data Node che rifanno la medesima operazione sui sottoinsiemi di dati partizionati che gli sono arrivati in input.  La catena split/map task/output gira su un singolo nodo, garantendo la località del dato, e senza necessità di replica dei dati in output, perché qualora ci fosse necessità di ricostruire l’output per il failure di un nodo, un altro nodo può ottenere facilmente lo split e rigenerarlo da capo; quindi non è necessario replicare i risultati intermedi dell’elaborazione.\n\nREDUCE\n\nGli output dei vari mapping vengono copiati su un singolo Data Node e a questi poi vengono applicati i reduce task, che vanno a calcolare il risultato finale dell’operazione. Questo procedimento è sottoposto a meccanismi di replica dell’HDFS, in modo che in caso di failure di un nodo, sia possibile ripristinare i dati importanti per le elaborazioni;\nSe le risorse di un singolo nodo non sono sufficienti a garantire il rispetto di alcune tempistiche di elaborazione delle operazioni di reduce, si può usare una molteplicità di nodi. Il numero di task di reduce non è direttamente correlato alla dimensione dell’input ma è indipendente; è strettamente correlato alle performance che si vogliono ottenere.\n\n\nRACK AWARENESS\n\n\n                  \n                  Important\n                  \n                \n\n\nper ridurre al minimo il trasferimento di blocchi (transfer rate) tra cluster. Lo scheduler MR utilizza anche la distanza per determinare dove è disponibile la replica più vicina per avviare le attività di map/reduce\nDi seguito è riportata la distanza tipica dei server\nD = 0 stesso nodo\nD = 2 distanza tra i nodi nello stesso rack\nD = 4 distanza tra i nodi in rack diversi\nD = 6 distanza tra i nodi di diversi data center\n\n\n\n\nIn Hadoop un concetto fondamentale nella gestione dei nodi è la definizione di prossimità dei nodi. In un’architettura distribuita è importante considerare come elemento chiave la larghezza di banda, intesa come il transfer-rate tra i nodi su cui girano i processi MapReduce. La larghezza di banda può rappresentare dunque una misura di distanza tra i nodi di elaborazione, ma questa non può essere facilmente misurata, poiché varia in funzione di cosa stanno eseguendo i processi e quali dati stanno prendendo, oltre alla località dei dati stessi. La larghezza di banda diventa sempre più piccola man mano che si passa da processi che girano su uno stesso nodo a nodi che risiedono sullo stesso rack, passando da nodi di rack diversi dello stesso data center, per poi finire a nodi che risiedono su diversi data center. Quindi si può considerare la topologia come elemento fondante di questa architettura. in Hadoop si è scelto di basare l’architettura su considerazioni topologiche, quindi su come è costruito il cluster distribuito.\nStep Invocazioni Hadoop\nLa Figura 2.21 mostra i demoni che controllano i passaggi nella sequenza di esecuzione di MR. NN gestisce il caricamento e la suddivisione dei big data in blocchi di uguali dimensioni. JT si occupa della gestione del ciclo di vita del lavoro e della formazione dell’IS. La funzione da RR a combinatore viene eseguita nella JVM dell’attività di mappatura. I passaggi (rimescolamento, unione, ordinamento, gruppo) sono gestiti dallo stesso framework di esecuzione MR. I passaggi rimanenti vengono eseguiti riducendo l’attività per terminare l’esecuzione del lavoro. Gli utenti possono definire quasi tutte le funzioni nella sequenza di esecuzione, ma solo le funzioni di mappatura, combinazione, partizionamento e riduzione sono abbastanza comunemente Personalizzate\n\nCOMPONENTI MR e HDFS\n\nNN: gestisce i metadati del file system, controlla e coordina i DN.\nDN: gestisce i dischi locali, archivia e recupera blocchi di dati su istruzioni NN.\nSNN: conserva una copia dei metadati da NN per evitare SPOF.\nJT: prepara il piano di esecuzione, avvia le attività di mappatura/riduzione ed esegue il coordinamento delle fasi.\nTT: gestisce CPU e memoria come slot, esegue attività di mappatura/riduzione\nClient HDFS: un demone che riceve la richiesta di caricamento dei dati e interagisce con NN per inserire blocchi di dati. Può essere eseguito in qualsiasi nodo del cluster Hadoop.\nJob Client: un demone che riceve il lavoro MR e interagisce con JT. Può essere eseguito in qualsiasi nodo del cluster Hadoop.\nAttività di mappatura: esegue una funzione di mappa definita dall’utente, che legge i dati da HDFS come coppie chiave/valore e produce un numero arbitrario di coppie chiave- valore intermedie\nAttività di riduzione: esegue una funzione di riduzione definita dall’utente, che raccoglie l’output da tutte le attività della mappa, unisce, ordina, raggruppa i valori che appartengono alla stessa chiave e infine produce un numero arbitrario di coppie chiave-valore.\n\nYARN\nLo YARN raggruppa le risorse del cluster e le condivide tra strumenti e framework. YARN è essenzialmente un sistema software per la gestione di diversi framework distribuiti. Gestisce e condivide le risorse del cluster in modo granulare (condividendo le risorse in qualsiasi proporzione in base alla sua disponibilità) tra diversi framework distribuiti nello stesso cluster per un migliore utilizzo del cluster. Lo YARN non sa che tipo di applicazione è in esecuzione su di esso. Può essere il lavoro di MR o Spark o Storm. YARN si basa sull’architettura master-slave e ha due componenti principali:\n\nYARN è il Resource Manager di Hadoop. In termini di layer applicativi (si veda la Figura 4) così come HDFS e HBASE sono a livello di Storage, YARN è a livello di Computing, mentre applicativi quali MapReduce o Spark1 utilizzano i servizi messi a disposizione da YARN per permettere ad ulteriori applicazioni di girare su di esse.\n\nI componenti di YARN sono: Client Node, Resource Manager, Node Managers e Container. Sui Client Node gira l’Application Client che usufruisce dei servizi Hadoop e YARN permettendo che tutto funzioni gestendo le risorse disponibili. Vediamo nel dettaglio cosa fa ogni componente, vedendo in pratica come funziona una elaborazione YARN.\n\nUn Application Client richiede l’esecuzione di una applicazione YARN al Resource Manager.\nIl Resource Manager a sua volta richiede l’istanziazione di un Container, in cui far girare i processi elaborativi, ad un Node Manager.\nIl Container avrà una Master Application (essendo la prima ad essere istanziata) che gira in esso. Il processo viene lanciato.\nPuò succedere che o il processo non ha necessità di ulteriori risorse di quelle disponibili nel Node Manager, oppure può richiedere al Resource Manager di allocare ulteriori risorse, che vengono comunicate dal Resource Manager al Node Manager della Master Application, la quale non farà altro che istanziare ulteriori container su ulteriori nodi, dove gireranno dei processi ulteriori che saranno istanziati volta per volta dai singoli Node Manager.\n\n\nResource Manager(RM)\nÈ un servizio principale e un gestore di risorse centralizzato nel cluster. Indica al node manager di avviare i contenitori per le attività di mappatura/riduzione. C’è solo un RM nel cluster. Le funzionalità specifiche di RM sono:\n• gestione e condivisione delle risorse del cluster.\n• gestire le richieste di risorse.\n• fornire sicurezza con Kerberos.\nNode Manager\nOgni nodo slave esegue un demone NM, che gestisce le risorse slave (memoria e CPU). Il NM comunica con RM per registrarsi far parte di un cluster YARN. Le responsabilità di NM sono la creazione, il monitoraggio e l’eliminazione dei Container per l’AppMaster di MR, l’esecuzione dei task di mappatura e riduzione.\nContainer\nIl container è un’unità base di allocazione delle risorse in YARN per qualsiasi applicazione/lavoro. Un contenitore è composto  da CPU virtuale e da una porzione di memoria per avviare attività di mappa/riduzione e l’AppMaster di MR. Un container è programmato da RM e supervisionato da NM. In MRv1, gli utenti possono definire il numero di slot di mappatura/riduzione e la sua configurazione in TT. Tuttavia, il grave svantaggio di uno schema basato sugli slot è che i servizi MR dovrebbero essere interrotti per riconfigurare gli slot. Inoltre, gli slot non sono specifici del lavoro. Ad esempio, considera uno slot della mappa configurato con 1 GB di memoria. Anche se un’attività di mappa richiede solo 200 MB, occupa l’intero 1 GB e non rilascerà la memoria inutilizzata fino al completamento o alla conclusione dell’attività. Pertanto, provoca il sottoutilizzo delle risorse. Al contrario, il contenitore in MRv2 è specifico del lavoro. Ad ogni lavoro può essere assegnata una dimensione diversa del contenitore. Ad esempio, il contenitore1 (2 GB di memoria, 1 core logico), il contenitore2 (3 GB di memoria e 2 core logici) possono avere una configurazione diversa. Tuttavia, il numero di contenitori possibili in un NM dipende dal numero di core e dalla dimensione della memoria dedicata a YARN nel particolare NM.\nMaster dell’applicazione MR (MRAppMaster)\nLo YARN fornisce un Application Master per ogni job di MR, Spark, Storm, ecc. Viene creato quando inizia un lavoro MR e distrutto quando termina un lavoro MR. Ogni lavoro MR ha il proprio MRAppMaster dedicato. Se nel cluster sono in esecuzione due lavori MR, verranno eseguiti due MRAppMaster. MRAppMaster gestisce il ciclo di vita del lavoro MR (in MRv1 JT gestisce il ciclo di vita del lavoro). Richiede a RM che i contenitori avviino attività di mappatura/riduzione. La risposta RM contiene informazioni sui container da lanciare. MRAppMaster coordina la mappatura/riduzione delle attività e delle fasi, aggrega registri e contatori dai NM. Si comporta come un JT di breve durata per ogni lavoro MR. Fornisce tolleranza agli errori per le attività di mappatura/riduzione. MRAppMaster stesso gestisce la maggior parte delle funzionalità MRv1\nfasi MapReduce\nL’esecuzione di un job MapReduce è seguito da diversi attori:\n\nil Client che sottomette il job il Resource Manager che gestisce l’allocazione di risorse\ni Node Managers che lanciano e controllano i Containers elaborativi\nl’Application Master che fa la coordinazione dell’esecuzione dei vari processi\nl’HDFS che rende possibile la condivisione dei vari job files.\nVediamo dunque come questi interagiscono tra di loro (si veda la Figura 8).\n\n\nIl Client si occupa di sottomettere un Job interagendo con il Resource Manager e l’HDFS (Figura 8a)\nIl Resource Manager inizializza un’Application Master su uno specifico Node Manager, e distribuisce le richieste di split sui nodi dell’HDFS (Figura 8b).\nEventualmente l’Application Master può richiedere l’assegnazione di ulteriori nodi elaborativi (Figura 8c), e quindi consentire l’esecuzione di processi figlio (che possono essere task di Map o di Reduce che interagiscono con l’HDFS) in ulteriori Node Manager (Figura 8c).\n\nQuindi sostanzialmente l’esecuzione di un Job di MapReduce si articolare in 4 diverse fasi:\njob submission\nviene invocato il metodo submit() che crea una istanza di JobSubmitter e chiama la classe di submitJobInternal; a questo punto la classe waitForCompletion() verifica lo stato e lo riporta alla console del job sottomesso: se il job è stato completato correttamente vengono visualizzati i job counters, altrimenti viene loggato l’errore verificatosi;\njob initialization\nquando la Job Submission richiede al ResourceManager una nuova AppId vengono verificate le specifiche di output, calcolati gli split di input da distribuire sul sistema HDFS, e copiate le risorse necessarie sul file system condiviso; quindi viene istanziato il ResourceManager invocando il metodo submitApplication();\ntask assignment\na questo punto può essere inizializzato il job, il ResourceManager chiama lo schedulatore YARN che alloca un container e lancia l’ApplicationMaster al suo interno su uno specifico NodeManager. L’ApplicationMaster viene inizializzato con gli oggetti che gestiscono il logging dell’esecuzione del job stesso, e a questo punto l’ApplicationMaster identifica dove si trovano tutti gli input split che sono stati calcolati dal Client, e crea un processo Map per ciascuno, in modo tale da rendere possibile al processo Map di leggere il DataSplit corrispondente;\ntask execution\na questo punto avviene la cosiddetta Uberization, ossia la valutazione delle risorse disponibili nel nodo dove risiede l’ApplicationMaster, per verificare che esse siano sufficienti all’esecuzione e al completamento del job. Se ciò non è possibile l’ApplicationMaster richiede i container che gli sono necessari per tutti i Map e i Reduce tasks al ResourceManager; tutti i Map tasks devono essere chiesti prima dei Reduce tasks, e tutte le richieste di Data Locality ( certi mapping e certi reduce possono avvenire specificatamente su certi nodi.) devono essere prese in considerazione dallo schedulatore. L’ApplicationMaster non fa altro che inizializzare un container in ciascun nodo che gli è stato reso disponibile dal ResourceManager. Prima che il task venga eseguito, tutte le risorse necessarie all’esecuzione sono collezionate e ciascun processo figlio viene ad essere eseguito in una JVM dedicata, in modo da evitare che possibili crash del job inficino la corretta esecuzione del NodeManager, al fine di massimizzare l’affidabilità del sistema, che è uno degli scopi principali dell’architettura Hadoop e di MapReduce.\nScheduler\nIl concetto fondamentale che è alla base di YARN è la gestione ottimale delle risorse disponibili (memoria, CPU e larghezza di banda), anche attraverso una opportuna schedulazione dei processi. Fondamentale è il concetto di Locality, intesa come la specifica di dove i container, quindi i processi elaborativi, devono essere eseguiti in termini di quale nodo e quale rack. Quindi è fondamentale avere chiaro quelle che sono le risorse disponibili, oltre alle richieste di localizzazione dei processi che devono essere istanziati. Questi vincoli di località sono importanti per poter utilizzare la larghezza di banda in maniera efficiente, oltre al fatto che bisogna considerare che le risorse possono essere chieste dalle applicazioni in maniera dinamica: Upfront quando vengono richieste tutte all’inizio (Spark), o Phased man mano che vi è necessità (MapReduce). Altro aspetto fondamentale riguarda la gestione della schedulazione nel tempo delle varie richieste fatte dalle applicazioni.\nL’obiettivo di qualsiasi allocazione di risorse è quello di gestirle secondo delle policy specifiche. In YARN si utilizza:\nFirst In First Out (FIFO)\nviene eseguita per prima la prima richiesta ricevuta, le altre vengono accodate. Ha il beneficio di non avere bisogno di nessuna configurazione, ma pecca nel caso in cui alcune applicazioni richiedano molte risorse per cui altre applicazioni dovranno attendere molto tempo;\n\n\n                  \n                  ESEMPIO FIFO \n                  \n                \n\n\nNel caso FIFO (si veda la Figura 6a) quando verrà lanciato un Job1 all’istante 0 questo prenderà tutte le risorse disponibili fino al suo completamento. Il Job2 e il Job3 lanciati in seguito devono attendere rispettivamente che i precedenti siano terminati per poter essere lanciati ed eseguiti;\n\n\n\nCapacity\nvengono create delle code dedicate, a ciascuna delle quali viene assegnata una certa capacità di elaborazione e memoria. Le code possono essere ulteriormente suddivise gerarchicamente. Ha il vantaggio di poter variare l’elasticità, quindi le risorse associate a ciascuna coda;\n\n\n                  \n                  ESEMPIO CAPACITY \n                  \n                \n\n\nNel caso Capacitivo (si veda la Figura 6b) vengono definite due code, QueueA e QueueB: alla QueueA viene assegnato il 75% delle risorse, il restante 25% viene assegnato alla QueueB. Quando viene lanciato il Job1 gli viene assegnata la QueueA, al Job2 viene assegnata la QueueB; il Job3 deve attendere che le risorse di una Queue siano liberate prima di poter essere eseguito.\n\n\n\nFair Scheduler\ntutte le applicazioni che girano hanno lo stesso share disponibile di risorse. Ha il vantaggio di poter definire code gerarchiche, e ad ognuna può essere attribuita una policy di scheduling di tipo capacitivo o di tipo FIFO.\nl Fair Scheduling risulta quindi essere uno dei modi più flessibile per effettuare la schedulazione di risorse, e offre una gestione più evoluta di un cluster condiviso, anche se può sembrare più complicato.\n\n\n                  \n                  ESEMPIO FAIR SCHEDULER \n                  \n                \n\n\nNel caso Fair (si veda la Figura 6c) vengono comunque definite delle Queue con quantità di risorse definita; a ciascun Job viene dato il giusto share delle risorse. All’inizio al Job1 vengono assegnate tutte le risorse (di entrambe le code, poiché non vi sono altri Job in esecuzione contemporaneamente); quando poi viene lanciato il Job2, a questo li vengono assegnate tutte le risorse della QueueB, e al Job1 devono essere re-istanziate le risorse della QueueA, perché è necessario che il Job2 giri. Quando viene lanciato il Job3 gli vengono assegnate metà delle risorse della QueueB (quindi vengono re-istanziate le risorse al Job2) e prende l’intera QueueB quando il Job2 finisce, e l’intero sistema di risorse quando termina anche il Job1.\n\n\n\n\nDominant Resource Fairness (DRF)\nAltro aspetto molto importante in un ambiente di gestione delle risorse condivise è la possibilità di bilanciare l’utilizzo di molteplici tipologie di risorse. YARN gestisce questo problema cercando di modulare l’utilizzo della risorsa dominante rispetto all’utilizzo complessivo del cluster: questo concetto è definito Dominant Resource Fairness (DRF). Vediamo un esempio concreto considerando la Figura 7.\n\n\n                  \n                  ESEMPIO \n                  \n                \n\n\nSi supponga di avere un cluster con 100 CPU e 10TB di memoria RAM. Se vi è una applicazione A che richiede 2 CPU e 300 GB di RAM sta richiedono il 2% di CPU, e il 3% di memoria RAM; quindi questa applicazione sta facendo una richiesta “Dominante in termini di memoria”. Una applicazione B che richiede 6 CPU e 100 GB di RAM sta richiedono il 6% di CPU, e l’1% di memoria RAM; quindi questa applicazione sta facendo una richiesta “Dominante in termini di CPU”. In termini di risorsa dominante quindi l’applicazione B sta richiedendo 2 volte la risorsa dominante rispetto all’applicazione A (6% rispetto al 3%). Quindi YARN allocherà all’applicazione B metà delle risorse che sta richiedendo (quindi 3 CPU e 50 GB di memoria), per poter modulare in maniera più efficiente ed opportuna le risorse tra le due applicazioni\n\n\n\n"},"IntroduzioBigData/Hadoop-and-MapReduce/MapReduce":{"slug":"IntroduzioBigData/Hadoop-and-MapReduce/MapReduce","filePath":"IntroduzioBigData/Hadoop&MapReduce/MapReduce.md","title":"MapReduce","links":["IntroduzioBigData/Hadoop-and-MapReduce/Hadoop","IntroduzioBigData/Big-Data/Big-Data","IntroduzioBigData/Hadoop-and-MapReduce/HDFS","IntroduzioBigData/Hadoop-and-MapReduce/MapReduce"],"tags":[],"content":"MR è uno strumento di elaborazione batch in parallelo dei dati altamente distribuito, scalabile orizzontalmente, con tolleranza agli errori nel framework Hadoop che viene eseguito su un cluster di server di base inaffidabili per elaborare Big Data. ==I dati vengono raccolti e archiviati su HDFS prima di avviare i lavori MR==.\nE’ facile sviluppare algoritmi scalabili utilizzando la MR senza ulteriori sforzi per gestire un sistema distribuito:\nUn programma sviluppato per un nodo può essere utilizzato per migliaia di nodi senza modificare nuovamente il codice.\nMR stesso parallelizza l’esecuzione, quindi gli utenti non devono investire sforzi per l’esecuzione parallela.\nLa programmazione MR si basa su LISP (LISt Programming), un tipo di paradigma di programmazione funzionale (come Haskell, Scala, Clojure, Smalltalk, Ruby). Tutto nella programmazione funzionale ruota attorno alla funzione. Una funzione può essere passata/ ricevuta come argomenti, distribuita tra i nodi. La programmazione funzionale non mantiene lo stato e non supporta il blocco e la sincronizzazione. Quindi, è scalabile.\nLe attività MR vengono eseguite in modo indipendente, quindi è facile gestire i guasti parziali. Per sperimentare la vera potenza della MR, si dovrebbe lavorare con set di dati in TB, perché è qui che RDBMS impiega ore e fallisce, mentre Hadoop fa lo stesso in pochi minuti. MR ha due sottocomponenti (vedi Figura 2.12): JT e TT. Un tipico lavoro MR esegue due attività: mappare e ridurre\n\nFASI MAP REDUCE\nIn generale, ci sono due fasi per l’esecuzione di un lavoro MR, come mostrato nella Figura 2.14: fase di mappatura e fase di riduzione.\n• La Fase MAP esegue una serie di attività di mappa (mapper) per leggere i dati dai dischi come coppie chiave-valore e produrre un numero arbitrario di coppie chiave-valore intermedie in base alla funzione di mappa definita dall’utente.\n• La fase di riduzione Fase Reduce esegue una serie di attività di riduzione (riduttori), che raccolgono l’output da tutte le attività della mappa, uniscono, ordinano in base alla chiave, aggruppano elenchi di valori che appartengono alla stessa chiave e producono l’output finale in base alla riduzione definita dall’utente funzione.\n\n\n\n\nFase MAP\nil client HDFS divide fisicamente il file di input inviato in blocchi di uguali dimensioni e lo archivia in DN diversi in base alla consapevolezza del rack(RACK AWARENESS).\nI blocchi di dati richiesti vengono portati in memoria per alimentare l’attività della mappa per l’esecuzione per iniziare la fase della mappa.\nLa fase della mappa inizia da FileInputFormat e termina quando tutte le attività della mappa sono state completate.\nIl nodo della mappa per un’attività della mappa esegue la seguente sequenza di funzioni da eseguire insieme alla funzione della mappa:\nDivisione input ⇒Lettore di record ⇒ Mapper ⇒ Partizionatore ⇒ Combinator\n\nInput Split (IS)\nUn IS rappresenta un insieme di blocchi che devono essere elaborati da una singola attività della mappa. Block è il concetto HDFS mentre IS è il concetto MR. IS è il raggruppamento logico di uno o più blocchi fisici. IS farà riferimento ad almeno un blocco. Per impostazione predefinita, la dimensione IS è uguale alla dimensione del blocco predefinita (64 MB). Come mostrato in figura 2.16, il file di input da 256 MB è diviso in quattro blocchi fisici da 64 MB e archiviati in DN diversi. Se la dimensione dell’IS è 128 MB, ogni IS raggruppa logicamente due blocchi fisici. Ogni IS viene elaborato da un’attività di mappa. Tieni presente che IS non contiene una copia dei blocchi fisici. Contiene solo la posizione dei blocchi fisici e i relativi metadati.\n\n\n\n                  \n                  Chiave-Valore \n                  \n                \n\n\nNell’ambiente di programmazione MR, ad ogni dato è associata una chiave. Una coppia chiave-valore è chiamata record. IS e i record sono entità logiche utilizzate al momento dell’esecuzione del lavoro. Non influenzano i blocchi fisici. Ogni passaggio nella sequenza di esecuzione MR accetta una coppia chiave-valore come input e restituisce coppie chiave-valore\n\n\n\nMappatura\nattività di mappatura Il mapping  è una funzione di mappa definita dall’utente, utilizzata principalmente per la pre-elaborazione dei record. Pertanto, l’output dell’attività di mappatura viene chiamato come coppie chiave-valore intermedie, ma non il risultato del lavoro MR.\nRR alimenta un record per mappare la funzione. Quando una funzione riceve input e produce output, viene chiamata task. Le attività di mappatura e riduzione vengono richiamate solo una volta al momento del lancio. Tuttavia, le funzioni map e reduce vengono richiamate per ogni record di input. Ogni attività della mappa ha il suo RR, come mostrato nello FASE MAPPA. Se sono presenti “n” record di input in IS, viene richiamata la funzione di mappa “n” volte e produce zero o più record di output. La funzione mappa consente agli utenti di scrivere la propria logica per decidere cosa fare con i record. È possibile analizzare il valore ed estrarre solo i campi rilevanti (proiezione) o filtrare i record indesiderati/errati o trasformare i record in entrata. L’output dell’attività della mappa viene archiviato in un buffer in memoria, come mostrato nella Figura 2.23. La fase della mappa termina solo dopo che tutte le attività di mappa di un lavoro sono state completate. La latenza di ciascuna attività della mappa può variare a causa di altre attività simultanee nel sistema\nPartizionamento (bilanciamento e riduzione dell’input dell’attività)\nSe viene avviata più di un’attività di riduzione, il partizionatore decide a quale attività di riduzione deve essere indirizzato un record di output della mappa. Il partizionatore è una funzione che bilancia la riduzione delle dimensioni dell’input delle attività da tutte le attività della mappa. Il partizionatore ha senso solo quando lanciamo più di un’attività di riduzione. Il partizionatore suddivide l’output di un’attività di mappa in più partizioni. Una partizione è una porzione dell’output della mappa che va a un particolare riduttore. Il numero di partizioni è uguale al numero di attività di riduzione. L’obiettivo del partizionamento è portare la stessa chiave da diverse attività della mappa un unico riduttore.\n\n\n                  \n                  Example\n                  \n                \n\n\nAd esempio, considera 10 attività sulla mappa per un lavoro di conteggio delle parole. Considera che le attività della mappa 1, 4, 6 e 10 producono la parola “Hadoop” come output. Per trovare il conteggio totale della parola “Hadoop”, è necessario ridurre un’attività. Pertanto, la parola “Hadoop” può essere contata come 4. Il partizionatore predefinito è HashPartitioner, che calcola il valore hash per decidere a quale riduttore deve essere inviato il record corrente. Funziona bene con qualsiasi numero di partizioni e garantisce che ciascuna partizione abbia il giusto mix di chiavi, portando a partizioni di dimensioni più uniformi.\n\n\n\nCombinatore (ottimizzazione IO di rete e disco)\nL’attività di combinazione riduce al minimo il traffico di rete, il trasferimento di I/O su disco e il numero di record elaborati dall’attività di riduzione. la funzione combinatore viene utilizzata per ridurre al minimo la dimensione dell’output della mappa localmente dopo la preparazione delle partizioni.\nNel contesto di MapReduce, il Combiner è un componente opzionale ma molto utile che agisce come un “mini-reducer” locale. Il suo scopo principale è quello di ridurre la quantità di dati intermedi generati dalle funzioni Map prima che vengano inviati alla fase di Shuffle e Sort e, successivamente, ai Reducer.\nEcco cosa fa e perché è importante:\n\nAgisce dopo il Mapper e prima dello Shuffle/Sort: Dopo che un Mapper ha elaborato il suo blocco di dati e ha emesso coppie &lt;chiave, valore&gt; intermedie, il Combiner interviene su queste coppie localmente, sullo stesso nodo del Mapper.\nRiduzione locale dei dati: Il Combiner esegue un’aggregazione o una riduzione parziale delle coppie &lt;chiave, valore&gt; che hanno la stessa chiave sul nodo locale. Questo significa che, invece di inviare tutte le singole coppie al Reducer, il Combiner riassume i dati, diminuendone il volume.\nMiglioramento delle prestazioni: Il vantaggio più significativo del Combiner è la riduzione del traffico di rete. Inviando meno dati attraverso la rete tra i Mapper e i Reducer, si alleggerisce la congestione e si velocizzano notevolmente i tempi di esecuzione complessivi del job MapReduce.\n“Mini-Reducer”: Spesso, la logica del Combiner è identica o molto simile a quella del Reducer. Questo perché il Combiner deve eseguire un’operazione che sia associativa e commutativa, in modo che l’ordine in cui i dati vengono combinati localmente non alteri il risultato finale prodotto dal Reducer. Se un’operazione non è associativa e commutativa (ad esempio, calcolare la media), usare un Combiner con la stessa logica del Reducer potrebbe portare a risultati errati.\n\n\n\n                  \n                  Example\n                  \n                \n\n\nEsempio pratico (Conteggio parole):\nMapper: “ciao mondo ciao” → &lt;“ciao”, 1&gt;, &lt;“mondo”, 1&gt;, &lt;“ciao”, 1&gt;\nCombiner (sul nodo del Mapper): Riceve &lt;“ciao”, 1&gt;, &lt;“mondo”, 1&gt;, &lt;“ciao”, 1&gt; e le aggrega localmente in &lt;“ciao”, 2&gt;, &lt;“mondo”, 1&gt;. Senza Combiner: Tutte le coppie originali sarebbero inviate ai Reducer. Vantaggio: Il Reducer finale riceverà meno dati da elaborare, poiché gran parte dell’aggregazione è già stata fatta a livello locale.\n\n\n\nFase Reduce\nLa funzione di riduzione in ciascuna attività di riduzione viene eseguita solo dopo che tutte le attività della mappa sono state completate. In generale, ridurre l’output degli spostamenti di fase (partizioni) di mapper/combinatore dai nodi della mappa per ridurre i nodi, **unisce tutte le partizioni, ordina in base alla chiave e raggruppa tutti i valori che appartengono alla stessa chiave per eliminare la ridondanza della chiave\n\nShuffle (fase di copia)\nShuffle è il processo mediante il quale l’output partizionato del mapper(Mappatura)/combinercombiner viene trasferito sulla rete HTTP a uno o più TT dove verranno ridotte le attività essere eseguito. Questa è anche chiamata fase di copia. Ciascun nodo di riduzione riceve una o più partizioni da tutte le attività della mappa. Se si decide di eseguire l’attività di riduzione nello stesso nodo in cui viene completata l’attività di mappa, lo shuffle non ha alcun ruolo. Ma come fa un nodo di riduzione a sapere quale nodo della mappa interrogare per le partizioni? Questo viene fatto con l’aiuto di JT. Al completamento di ogni attività della mappa, notifica al JT le partizioni. Ogni riduttore interroga periodicamente JT per conoscere il nodo che esegue le attività della mappa. Considera un file di input da 2 TB. Cosa succede se la dimensione dell’output del mapper è la stessa dell’input? Lo spostamento di 2 TB per ridurre i nodi sulla rete richiede un’enorme larghezza di banda. Questo è il motivo per cui il combinatore combiner viene eseguito per ridurre al minimo la dimensione dell’output della mappa da spostare attraverso una rete per raggiungere il nodo ridotto. Per ridurre ulteriormente le dimensioni dell’output della mappa, è possibile comprimerla. Snappy è il compressore più comunemente utilizzato in MR.\nUnisci e Ordina (merge sort)\nLe rispettive partizioni di output della mappa vengono copiate nell’attività di riduzione Java Virtual Machine (JVM). Non appena arrivano le partizioni da tutte le attività della mappa per ridurre i nodi, le partizioni dovrebbero essere unite in un unico file per l’ulteriore elaborazione. Ogni 10 file distribuiti vengono uniti per impostazione predefinita.\nIl file unito deve essere ordinato in base alla chiave.\nGruppo (Group)\nUn insieme di valori che appartengono alla stessa chiave viene raggruppato per eliminare la ridondanza delle chiavi. Il numero di volte in cui viene eseguita la funzione di riduzione è uguale al numero di coppie chiave:elenco(valori) dopo il raggruppamento. Se esistono chiavi duplicate, la funzione di riduzione viene ripetutamente richiamata per ogni chiave duplicata. Se la stessa chiave è presente in più file distribuiti, per una singola chiave è necessario inserire molti file memoria (richiede più passaggi) per alimentare la funzione di riduzione. Ciò comporta molto IO. Questo è il motivo per cui ordiniamo e raggruppiamo i record prima di chiamare la funzione di riduzione. Pertanto, è richiesto un solo passaggio per ciascuna chiave e viene richiamata una sola volta la funzione di riduzione per ciascuna chiave univoca.\nridurre l’attività (reducer)\nLa funzione Riduci elabora un elenco di valori per ciascuna chiave e produce zero o più record di output. Il numero di volte in cui viene eseguita la funzione di riduzione è uguale al numero di record (chiave: elenco di valori) dopo il raggruppamento. La funzione mappa viene utilizzata principalmente per la pre-elaborazione dei record. Tuttavia, l’algoritmo principale è implementato nella funzione di riduzione. Le operazioni di aggregazione e unione vengono eseguite qui. L’output della mappa viene eliminato dopo il completamento con successo di tutti i riduttori. Se un nodo che esegue l’attività di mappa fallisce prima che l’output della mappa venga acquisito dalle attività di riduzione, il framework eseguirà automaticamente nuovamente l’attività di mappa per creare nuovamente l’output della mappa. JT decide in quale nodo devono essere eseguite le attività di riduzione. Potrebbe essere lo stesso nodo in cui vengono eseguite le attività della mappa o qualche altro nodo nello stesso rack o nodo in qualche altro rack. Dipende dalla disponibilità degli slot e dal carico della rete locale. Gli utenti possono specificare il numero di attività di riduzione in base ai requisiti di parallelismo. Se non specificata, viene avviata un’attività di riduzione predefinita, che utilizza l’output della mappa e i risultati così come sono, ma in modo ordinato mentre passa attraverso le funzioni di shuffle, sort e group. Se si imposta il numero di attività di riduzione su zero, l’output della mappa stesso viene considerato output del lavoro e verrà archiviato in HDFS. In questo caso non saranno previsti processi di shuffle, ordinamento, unione e raggruppamento. L’output dell’attività di riduzione viene generalmente archiviato in HDFS per impostazione predefinita con la replica, a differenza dell’output della mappa archiviato in memoria o distribuito nel file system locale. **La prima copia viene archiviata localmente dove è in esecuzione l’attività di riduzione, la seconda copia viene archiviata in qualsiasi nodo dello stesso rack e la terza copia viene archiviata in qualsiasi nodo di qualche altro rack nel cluster. Pertanto, la scrittura e la riduzione dell’output consumano la larghezza di banda della rete tanto quanto consuma la normale pipeline di scrittura HDFS.\nscrittura di record (RW)\nLa funzione di riduzione fornisce la chiave di output e il valore di output a RW, che a sua volta scrive su HDFS con separazione di tabulazione per impostazione predefinita in base a TextOut-putFormat. Ogni riduttore scrive un file di output su HDFS con la RF desiderata. RW apre un file di output e scrive i record di output ridotti. **L’output delle attività di mappa viene riversato nel file system locale perché la scrittura di risultati intermedi su HDFS porterà a repliche non necessarie e richiederà lavoro aggiuntivo per eliminarli in seguito. Tuttavia, l’output dell’attività di riduzione viene scritto su HDFS poiché è il risultato finale e dovrebbe essere con tolleranza agli errori."},"IntroduzioBigData/Machine-Learning/ML-NON-Supervisionato/Clustering-Gerarchico":{"slug":"IntroduzioBigData/Machine-Learning/ML-NON-Supervisionato/Clustering-Gerarchico","filePath":"IntroduzioBigData/Machine Learning/ML NON Supervisionato/Clustering Gerarchico.md","title":"Clustering Gerarchico","links":[],"tags":[],"content":"Il Clustering Gerarchico Agglomerativo è una tecnica di clustering utilizzata per organizzare i dati in una struttura ad albero, chiamata dendrogramma, che rappresenta le relazioni di similarità tra i dati.\nCaratteristiche principali:\n\nAgglomerativo: si considera ciascun data item come un cluster individuale, e ad ogni passo si uniscono le coppie di cluster che sono più vicine, finché non si trova un unico cluster, o il numero k di cluster che si vogliono considerare;\nDivisivo: si parte da un unico cluster inclusivo di tutti i data item e ad ogni passo si elimina da ogni cluster il punto più distante finché ogni cluster contiene un solo data point, o si identificano k cluster.\n\n➢ Distanza e Similarità: La fusione tra cluster è basata su una misura di distanza, come la distanza Euclidea o Manhattan, o su una misura di similarità.\n➢ Dendrogramma: La struttura gerarchica creata aiuta a visualizzare i cluster e a scegliere il numero ottimale di gruppi.\nLe distanze che possono essere utilizzate in un algoritmo di cluster gerarchico sono:\nSingle Linkage (Collegamento Singolo): calcola la distanza minima tra i punti di due cluster. La distanza tra due cluster è data dalla distanza più breve tra i singoli punti in ciascun cluster. È sensibile al rumore e tende a formare cluster “a catena”.\nComplete Linkage (Collegamento Completo): calcola la distanza massima tra i punti di due cluster. La distanza tra due cluster è quindi data dalla distanza maggiore tra i punti nei due cluster. Questo metodo tende a creare cluster più compatti e di forma simile.\nAverage Linkage (Collegamento Medio): misura la distanza media tra tutti i punti di un cluster e tutti i punti di un altro cluster. Spesso è una scelta bilanciata per ottenere cluster di forma uniforme.\nCentroid Linkage (Collegamento del Centroid): calcola la distanza tra i centroidi (o medie) di ciascun cluster. È sensibile alle variazioni nei centroidi e funziona bene se i cluster hanno distribuzioni simmetriche.\n"},"IntroduzioBigData/Machine-Learning/ML-NON-Supervisionato/Elbow-criteria":{"slug":"IntroduzioBigData/Machine-Learning/ML-NON-Supervisionato/Elbow-criteria","filePath":"IntroduzioBigData/Machine Learning/ML NON Supervisionato/Elbow criteria.md","title":"Elbow criteria","links":[],"tags":[],"content":"L’obiettivo della cluster analysis è quello di minimizzare la varianza (Var_w) all’interno del cluster (Within cluster variance), e di massimizzare la varianza (Var_b) tra i cluster (Between cluster variance).  In termini matematici si ottiene:\nVar_w(X)=E[(X-µ)^2]                   Var_b(Y)=E[(Y-µ)^2]\nW_k=Var_w/Var_b\nSi sta dunque cercando di accorpare quanto più possibile la presenza di punti intorno ad un centroide che rappresenta un cluster, mantenendo il più possibile separati questi punti da quelli appartenenti ad altri cluster. Quello che si fa per scegliere k è di adottare il cosiddetto “Criterio del Gomito”: all’aumentare del numero k di cluster ovviamente il rapporto W_k diminuisce. Ad un certo punto il guadagno marginale che si ottiene aggiungendo un nuovo cluster diminuirà drasticamente, generando un angolo nel grafico che rappresenta il rapporto W_k e il numero di cluster. Graficamente la situazione è quella mostrata sotto:\n\nDunque all’aumentare del numero di cluster k il rapporto tra le varianze Wk diminuisce, quindi il guadagno che si ottiene aggiungendo un nuovo cluster diminuisce nel tempo fino a divenire trascurabile. Quindi si può scegliere il numero di cluster k che meglio rappresentano in modo congruo ed omogeneo l’insieme di dati che si ha a disposizione."},"IntroduzioBigData/Machine-Learning/ML-NON-Supervisionato/Indice-di-Davies-Bouldin":{"slug":"IntroduzioBigData/Machine-Learning/ML-NON-Supervisionato/Indice-di-Davies-Bouldin","filePath":"IntroduzioBigData/Machine Learning/ML NON Supervisionato/Indice di Davies-Bouldin.md","title":"Indice di Davies-Bouldin","links":[],"tags":[],"content":"Un altro indice utilizzato che offre un’indicazione più concreta sul numero di cluster k da considerare è l’Indice di Davies-Bouldin: si effettua la media delle distanze D_{i} , rappresentate dal massimo del rapporto tra la separazione interna al cluster i e il cluster j (S_{i} e S_{j}), rispetto alla separazione tra il cluster i e il cluster j (M_{i,j}). L’indice DB non fa altro che trovare la media di tutti i massimi, andando quindi a rappresentare la situazione di scattering interno (tra i vari cluster) e lo scattering tra coppie di cluster. In formula:\n{\\displaystyle R_{i,j}={\\frac {S_{i}+S_{j}}{M_{i,j}}}}\n{\\displaystyle D_{i}\\equiv \\max _{j\\neq i}R_{i,j}}\nIf N is the number of clusters:\n{\\displaystyle {\\mathit {DB}}\\equiv {\\frac {1}{N}}\\displaystyle \\sum _{i=1}^{N}D_{i}}"},"IntroduzioBigData/Machine-Learning/ML-NON-Supervisionato/Kmeans":{"slug":"IntroduzioBigData/Machine-Learning/ML-NON-Supervisionato/Kmeans","filePath":"IntroduzioBigData/Machine Learning/ML NON Supervisionato/Kmeans.md","title":"Kmeans","links":["IntroduzioBigData/Machine-Learning/Statistica/Distanza","IntroduzioBigData/Machine-Learning/ML-NON-Supervisionato/Elbow-criteria","tags/","IntroduzioBigData/Machine-Learning/ML-NON-Supervisionato/Indice-di-Davies-Bouldin"],"tags":[""],"content":"\nL’obiettivo che l’algoritmo si prepone è di minimizzare la varianza totale intra-gruppo; ogni gruppo viene identificato mediante un centroide \\mu_K o punto medio.\nL’algoritmo  divide datset composto da N misure (o osservazioni) (x1,x2…xn)   in K clusters (gruppi) con N&gt;&gt; K seguendo una procedura iterativa:\n\nScegliere il numero di cluster k che si vogliono trovare;\nGenerare k punti casuali come centroidi \\mu_K  di ciascun cluster (rappresentativi del centro di ciascun cluster);\nEvaluation-step (E-step): Assegnare ciascun data point al centroide del cluster più vicino (in termini di distanza utilizzando una Funzione di similarità);\nMinimization-step (M-step): Ricalcolare il nuovo centroide del cluster;\nLoop di Evaluation-step (E-step) e Minimization-step (M-step) Ripetere il passo 3 e 4 finché non si converge\n\nFunzione di similarità\nLa funzione più semplice è la Distanza : più un elemento è distante, più è dissimile da un altro, più è vicino più è simile. Di solito si utilizza la distanza euclidea (bisogna considerare che i dati devono essere scalati, quindi omogenei dal punto di vista della distanza: possono esserci dati che hanno unità di misura diverse le une dalle altre, quindi potrebbero aversi fattori di\ndistanza più o meno significativi; quindi si ricorre ad una normalizzazione dei dati di input..)\nFunzione obiettivo/costo\nLa funzione obiettivo del K-Means, che l’algoritmo cerca di minimizzare iterativamente, è la Somma dei Quadrati all’interno dei Cluster (Within-Cluster Sum of Squares - WCSS), a volte chiamata anche “inerzia”. La sua formula è:\nJ=\\sum_{i=0}^{n}||x_i - \\mu_j||^2\nQuesta funzione J viene ad essere minimizzata nei  passi successivi:\nEvaluation-step (E-step):\n\nper ogni punto dati nel dataset, l’algoritmo calcola la distanza euclidea  tra quel punto e ogni centroide esistente.\nIl punto viene poi assegnato al cluster il cui centroide risulta essere il più vicino (quello con la distanza minima). Questa è   una decisione “greedy” (locale) per ogni singolo punto. Quindi  fissato il centroide µk si minimizza J  assegnando ciascun data item al suo centroide più vicino\n\nMinimization-step (M-step):\nfissata l’assegnazione dei data item ai centroide nel passo precedente, si effettua una nuova minimizzazione di J ricentrando µk in modo tale che ciascun centroide venga ad essere la media dei punti che appartengono al cluster K (crea nuovi centroidi prendendo il valore medio di tutti i campioni assegnati a ciascun centroide precedente.)\n\nUna volta che tutti i punti sono stati assegnati a un cluster, l’algoritmo ricalcola la posizione di ciascun centroide. Il nuovo centroide di un cluster è la media (o baricentro) di tutti i punti che sono stati assegnati a quel cluster.\nin questa fase che si vede la connessione con la Funzione obiettivo/costo: si può dimostrare matematicamente che il centroide calcolato come media dei punti di un cluster è la posizione che minimizza la somma dei quadrati delle distanze dei punti a quel centroide specifico.\n\nLoop di Evaluation-step (E-step) e Minimization-step (M-step)\nQuesta procedura viene ripetuta ciclicamente fino a convergere, sempre e comunque, ad un minimo locale.\n\\sum_{i=0}^{n}\\min_{\\mu_j \\in C}(||x_i - \\mu_j||^2)\nIn sintesi:\n\nAssegnazione dei punti: Avviene misurando la distanza euclidea  (con la radice quadrata) di ciascun punto da ogni centroide e assegnando il punto al centroide che restituisce la distanza minima.\nMinimizzazione della funzione obiettivo: La funzione obiettivo (WCSS, Somma dei Quadrati all’interno dei Cluster, che usa la distanza Euclidea al quadrato) viene minimizzata indirettamente attraverso l’alternanza di queste due fasi. Ogni passo (sia l’assegnazione che il ricalcolo dei centroidi) garantisce che il valore della WCSS non aumenti, e tipicamente diminuisca, portando l’algoritmo verso un minimo locale della funzione obiettivo.\n\nQuesto fa si che si riesca a garantire la convergenza in un numero finito di passi. I punti iniziali vengono scelti in maniera euristica, casuale: l’idea è quella di scegliere il centroide i+-esimo in modo da essere più possibile lontano dai primi. I limiti di questa scelta sono dati dai limiti spaziali dei data points nelle n dimensioni che abbiamo a disposizione.\nAltro aspetto fondamentale è quello di scegliere il numero k di cluster:\n\n\nElbow criteria\n\n Indice di Davies-Bouldin\n"},"IntroduzioBigData/Machine-Learning/ML-NON-Supervisionato/ML-Non-Supervisionato":{"slug":"IntroduzioBigData/Machine-Learning/ML-NON-Supervisionato/ML-Non-Supervisionato","filePath":"IntroduzioBigData/Machine Learning/ML NON Supervisionato/ML Non Supervisionato.md","title":"ML Non Supervisionato","links":["clustering","Hard-Clustering","Soft-Clustering","IntroduzioBigData/Machine-Learning/ML-NON-Supervisionato/Kmeans","IntroduzioBigData/Machine-Learning/ML-NON-Supervisionato/Clustering-Gerarchico","IntroduzioBigData/Machine-Learning/ML-NON-Supervisionato/ML-Non-Supervisionato","IntroduzioBigData/Machine-Learning/ML-NON-Supervisionato/PCA"],"tags":[],"content":"\nclustering\nL’analisi dei cluster (cluster analysis) è una tecnica statistica multivariata il cui obiettivo è raggruppare oggetti in base a un insieme di caratteristiche selezionate dall’utente.\nL’analisi dei cluster consente di dividere un insieme di osservazioni in cluster (sottoinsiemi) in modo tale che:\n➢ le osservazioni simili (o correlate) tra loro si trovano nello stesso gruppo\n➢ le osservazioni dissimili (o non correlate) rispetto agli oggetti si trovano in altri gruppi\nOgni gruppo contiene una serie di elementi disgiunti. Esistono due diverse tipologie di clustering:\n•Hard-Clustering, ciascun data-item xi appartiene ad un solo cluster Cj, quindi i cluster sono mutuamente esclusivi;\n•Soft-Clustering, ciascun data-item ha un grado di appartenenza (γki) ad un cluster. in cui γki rappresenta il grado di appartenenza del punto i. Questo è tipicamente associato ad un modello probabilistico: più alto è γki, più alta è la probabilità che il punto i appartenga al cluster k.\nCome Funziona la Cluster Analysis?\nPassaggi Principali:\n➢ Selezione delle Caratteristiche: Si scelgono le variabili o caratteristiche da analizzare, ad esempio età + glicemia + BMI + ….\n➢ Calcolo della Similarità/dissimilarità: Si misura la similarità tra le osservazioni usando metriche come la distanza euclidea utilizzando una funzione di dissimilarità, per scoprire quanto siano dissimili l’uno dall’altro, oltre ad una funzione di perdita/costo che consenta di valutare i vari cluster, e un algoritmo per ottimizzare la funzione di perdita/costo.\n➢  Algoritmo di Clustering: Si applica un algoritmo per dividere i dati in cluster.\nUna caratteristica degli algoritmi di clustering è quella di avere molte e diverse funzioni di costo (probabilistiche o non), e ciascuna di esse caratterizza l’algoritmo di clustering. Avendo a disposizione un dataset D quello che il clustering cerca di fare è andare a definire un numero k di insiemi disgiunti che rappresentano le caratteristiche peculiari del dataset, e per questo si tratta di un algoritmo che aiuta a fare Data Exploration, ad esplorare e comprendere l’insieme dei dati che si hanno a disposizione\n\nMETODI DI CLUSTERING\n➢ Kmeans: è un metodo che raggruppa i punti dati in un numero predefinito a priori (k) di cluster basato sulla loro distanza dal centroide di ciascun cluster. Si tratta di un algoritmo iterative.\n➢ Clustering Gerarchico: è un metodo che crea una gerarchia di cluster suddividendoli o unendoli ricorsivamente in base alle loro somiglianze.\nUna delle diverse tipologie di ML Non Supervisionato è la Principal Components Analysis (PCA)"},"IntroduzioBigData/Machine-Learning/ML-NON-Supervisionato/PCA":{"slug":"IntroduzioBigData/Machine-Learning/ML-NON-Supervisionato/PCA","filePath":"IntroduzioBigData/Machine Learning/ML NON Supervisionato/PCA.md","title":"PCA","links":["IntroduzioBigData/Machine-Learning/ML-NON-Supervisionato/ML-Non-Supervisionato","IntroduzioBigData/Machine-Learning/Statistica/Varianza","IntroduzioBigData/Machine-Learning/Statistica/matrice-di-covarianza","IntroduzioBigData/Machine-Learning/Statistica/Standardizazione"],"tags":[],"content":"\nUna delle diverse tipologie di ML Non Supervisionato è la Principal Components Analysis (PCA). La PCA ha l’obiettivo di derivare, dato un insieme di variabili p su un certo numero n di osservazioni, un set di features con dimensione minore. Quindi date n osservazioni in uno spazio p  dimensioni siano interessanti tutte allo stesso modo. Questo può succedere perché ci sono delle variabili che dipendono dalle altre, e possono essere ottenute come combinazione di un sottoinsieme di variabili. Dunque si cerca il set minimo di variabili che descrivono ugualmente bene, o nei limiti di una certa Varianza, l’intero set n di osservazioni date. La PCA vuole fare una valutazione alquanto ==geometrica: ridurre la dimensione di una data matrice nxp identificando le direzioni delle features nello spazio p dimensionale attraverso cui i dati originali (quindi le n osservazioni) siano altamente variabili(con la più alta Varianza==) .\n\nLa prima componente principale altro non è che la combinazione lineare normalizzata delle features, quindi delle p colonne della matrice X che ha la più ampia varianza\nLe p nuove variabili sono dette componenti principali (o principal components).\n➢ Caratteristiche delle componenti principali:\n▪ Le componenti principali sono nuove variabili, create artificialmente, nessuna di loro coincide con alcuna delle m variabili di partenza.\n▪ Ogni componente principale è una combinazione lineare delle m variabili originali.\n▪ Le componenti principali sono tali da riassumere quanta più informazione possibile sulle m variabili originali. **\n▪ Le componenti principali sono ordinate in base a quanta informazione del dataset originale racchiudono (𝑃𝐶1 è la componente più informativa, 𝑃𝐶2 la seconda più informativa ecc.)\n▪ Le componenti principali per costruzione sono tra loro scorrelate.\nIn pratica la PCA effettua una proiezione ortogonale dei dati su uno spazio definito da nuove dimensioni dette componenti principali. Queste sono tali per cui la varianza delle coordinate dei dati proiettati sulle nuove dimensioni è massima per le prime dimensioni.\n\nPerchè utilizzare la PCA?\nVisualizzazione\nQuando abbiamo dataset con tante variabili (m grande) diventa difficile rappresentare graficamente i dati per analizzarli dal punto di vista visivo.\n▪ Tipicamente si visualizzano le distribuzioni delle singole variabili o al più gli scatterplot di coppie di variabili.\n▪ Problemi:\n• Lo scatterplot di due variabili rappresenta solo una piccola quantità dell’informazione contenuta nei dati.\n• Con m variabili dovremmo fare 𝑚 ∙ (𝑚 − 1)/2 scatterplot. Se m = 10 → 10 x 9/2 = 45 scatterplot!\n➢ Possiamo sfruttare la PCA per riassumere l’informazione contenuta nei dati usando poche variabili, più semplici da rappresentare.\n▪ Potremmo realizzare lo scatterplot delle prime 2 componenti principali, 𝑃𝐶1 e 𝑃𝐶2, ovvero quelle più informative\nCompressione Dati\n➢ Compressione: al posto di archiviare le m variabili di partenza, archivio le p componenti principali, con un risparmio in memoria.\n➢ Decompressione: utilizzando le p componenti principali ricostruisco le m variabili di partenza. La ricostruzione non sarà perfetta, l’errore introdotto dalla compressione dipende da quanto informative erano le p componenti principali selezionate\nPCA, COME SI REALIZZA?\n➢ Dataset originale: m variabili, 𝑋1, 𝑋2, … , 𝑋𝑚, a media nulla.\n➢ Nota: se le variabili non sono a media nulla, prima di realizzare la PCA, i dati vanno centrati → ad ogni variabile va sottratta la sua media.\n\n➢ Trasformazione lineare normalizzata delle m variabili → m nuove variabili, 𝑃𝐶1, 𝑃𝐶2, …,𝑃𝐶𝑚, dette componenti principali tra loro scorrelate\n𝑃𝐶1 = 𝑣11 𝑋1 + 𝑣21 𝑋2 + … + 𝑣𝑚1 𝑋𝑚\n𝑃𝐶2 = 𝑣12 𝑋1 + 𝑣22 𝑋2 + … + 𝑣𝑚2 𝑋𝑚\n𝑃𝐶𝑚 = 𝑣1𝑚 𝑋1 + 𝑣2𝑚 𝑋2 + … + 𝑣𝑚𝑚 𝑋𝑚\n➢ I coefficienti 𝑣𝑖𝑘 si dicono loadings consentono di trasformare le variabili di partenza nelle nuove variabili, le componenti principali.\nI loadings devono essere tali che le prime componenti principali riassumano quanta più varianza possibile delle variabili di partenza:\n𝑣𝑎𝑟 𝑃𝐶1 &gt; 𝑣𝑎𝑟 𝑃𝐶2 &gt; 𝑣𝑎𝑟 𝑃𝐶3 &gt; ⋯ &gt; 𝑣𝑎𝑟 𝑃𝐶𝑚\n▪ 𝑃𝐶1  da sola deve essere in grado di spiegare quanta più varianza possibile dei dati di partenza.\n▪ 𝑃𝐶2 da sola deve essere in grado di spiegare quanta più varianza possibile della porzione di varianza non spiegata da 𝑃𝐶1.\n▪ …\n➢ Considerando solo le prime p componenti principali possiamo efficacemente ridurre la dimensionalità dei dati → nuovo set di p&lt;m variabili che riassume la maggior parte della Varianza delle m variabili di partenza.\nPer passare dalle coordinate dei punti nello spazio originale a quelle nello spazio definito dalle componenti principali, basta applicare la trasformazione lineare definita dai loadings\n\n𝒀  = 𝑿 ∙ 𝑽\n➢ La matrice V è di fatto una matrice di rotazione che consente di passare dalle coordinate nel sistema di riferimento originale, alle coordinate nel sistema di riferimento delle componenti principali.\nLa PCA ci fornisce dunque una nuova rappresentazione dei dati nello spazio delle componenti principali, 𝑃𝐶1, 𝑃𝐶2, …,𝑃𝐶𝑚, che ha 2 caratteristiche fondamentali:\n▪ Le componenti principali sono scorrelate\n▪ La varianza dei dati proiettati lungo le componenti principali decresce all’aumentare delle componenti, la maggior parte della varianza complessiva è concentrata nelle prime componenti principali 𝑣𝑎𝑟 𝑃𝐶1 &gt; 𝑣𝑎𝑟 𝑃𝐶2 &gt; 𝑣𝑎𝑟 𝑃𝐶3 &gt; ⋯ &gt; 𝑣𝑎𝑟 𝑃𝐶𝑚\n➢ Per ridurre la dimensionalità possiamo considerare solo le prime p componenti principali\nCOME SI CALCOLANO I LOADINGS?\nCome possiamo calcolare i valori dei loadings, ovvero la matrice V, che mi consente di realizzare la PCA?\n➢ Le colonne della matrice V sono gli autovettori della matrice di covarianza di X ordinati secondo l’ordine decrescente dei rispettivi autovalori.\nCalcolo della matrice di covarianza di X: S\n➢ Calcoliamo autovalori e autovettori di S.\nSia 𝑨 ∈ ℝ 𝑁×𝑁 una matrice quadrata di 𝑁 righe e 𝑁 colonne. Se esistono un vettore 𝒗 ∈ ℝ𝑁 e uno scalare 𝜆 (anche complesso) tali che:\n𝑨𝒗 = 𝜆𝒗 si dice che 𝒗 è autovettore di 𝑨 e 𝜆 il suo autovalore corrispondente.\nProprietà:\n➢ Una matrice di dimensione N x N ha al massimo N autovalori distinti.\n➢ Gli autovalori sono gli zeri del polinomio caratteristico:\ndet (𝑨 − 𝜆𝑰 )= 0\nGli autovettori di S (matrice di covarianza) rappresentano i loadings che definiscono le componenti principali.\n➢ Le componenti principali sono ortogonali tra loro → quindi scorrelate.\n➢ In che ordine vengono considerati gli autovettori per definire le componenti principali? → In base agli autovalori corrispondenti.\n➢ Ordiniamo i gli autovalori dal più grande al più piccolo:\n𝜆1 &gt; 𝜆2 &gt; ⋯ &gt; 𝜆𝑚\n|.        |               |\n𝒗𝟏      𝒗𝟐 …       𝒗𝒎\n➢ L’autovettore 𝒗𝟏 corrispondente all’autovalore massimo, 𝜆1, rappresenta il vettore dei loadings della prima componente principale, ovvero la prima colonna di 𝑽.\n➢ Gli autovettori, 𝒗𝟏, 𝒗𝟐, … , 𝒗𝒎, in questo ordine definiscono le colonne di 𝑽: 𝑽=[𝒗𝟏 𝒗𝟐 … 𝒗𝒎]\nGli autovalori rappresentano la varianza dei dati proiettati lungo le componenti principali\nPCA - RIASSUNTO\n\nCentramento (ad ogni colonna di X si sottrae la sua media) o  Standardizazione dei dati:\nCalcolo della matrice di covarianza di X → S (m x m)\nCalcolo di autovettori e autovalori di S costruendo la matrice V dei loadings.\n▪ Gli autovettori rappresentano i coefficienti per definire le componenti principali.\n▪ L’ordine delle componenti principali è stabilito dall’ordine degli autovalori.\n▪ L’autovettore corrispondente all’autovalore massimo rappresenta i coefficienti (loadings) della prima componente principale.\nTrasformazione dei dati passando alle coordinate nello spazio delle componenti principali: Y=X·V\nScelta delle prime p componenti principali che rappresentano gran parte della varianza delle variabili originali (scree plot o plot della frazione di varianza spiegata dalle prime p componenti).\nLe coordinate delle n osservazioni lungo le p componenti principali rappresentano il dataset trasformato e ridotto.\n\nSCELTA DEL NUMERO DI COMPONENTI PRINCIPALI (SCREE PLOT)\n"},"IntroduzioBigData/Machine-Learning/ML-Supervisionato":{"slug":"IntroduzioBigData/Machine-Learning/ML-Supervisionato","filePath":"IntroduzioBigData/Machine Learning/ML Supervisionato.md","title":"ML Supervisionato","links":["Classificazione","Regressione","Metodi-parametrici","Metodi-non-parametrici","80-percent","dataset-di-test"],"tags":[],"content":"Dati due vettori di elementi xi e yi, con i= 1, … , n si vuole apprendere una funzione f : X →Y che permette ottenere Y a partire da X, dove:\n• X è il vettore degli attributi di input, altrimenti noti come features;\n•Y è il vettore delle labels, che si vuole andare ad ottenere come output dell’algoritmo;\n\n*Classificazione\n\nNel caso di Y che contiene dati categorici e/o qualitativi, quindi appartenenti ad un dominio discreto (numerico o tipicamente testuale), si parla di Problemi di Classificazione; richiede campioni di riferimento precedentemente classificati (a priori) per addestrare il classificatore e successivamente classificare i dati sconosciuti\n\n\n*Regressione\n\nSe Y è un vettore rappresentato da dati puramente numerici e/o quantitativi in dominio continuo\n\n\n\nImportante è comprendere i metodi con cui si muove il dominio del ML supervisionato. Esistono:\n•Metodi parametrici, in cui si definisce la forma funzionale della funzione f (X) e si cerca il modello che meglio si adatta ai dati;\n•Metodi non parametrici in cui si cerca la miglior funzione f (X) che si adatta ai dati, senza però dare alcuna assunzione a priori circa il tipo o la forma della funzione f (X) stessa (Un esempio di un metodo non parametrico è il metodo dei K-Nearest Neighbors, nel quale si trovano i punti che appartengono ad una certa tipologia di classificazione andando ad identificare i punti di quella classifica che sono più vicini al punto stesso. Si fanno considerazioni indipendenti dalla definizione di una funzione, ma caratterizzate solo dalla presenza di quel punto in un certo spazio e da ciò che esso ha intorno in termini di caratterizzazione del vettore Y)\n\n\n                  \n                  Mean Squared Error (MSE) \n                  \n                \n\n\nE’ essenziale considerare che, quando si trova la funzione f (X) che permette di ottenere, a partire dal vettore X il vettore delle labels Y questo lo si fa con un ==errore ε dovuto all’approssimazione (Y= f (X) + ε). In questi casi bisognerà valutare il modello usando una metodologia, quale ad esempio la Mean Squared Error (MSE)==\n\n\n\nPer verificare la validità di questi algoritmi si va a dividere il dataset completo in un [dataset di training ] e un dataset di test (20%): si fa girare il modello cercando quali siano i parametri migliori sul dataset di training, e poi lo si applica su quello di test per verificare che i parametri prima applicati siano ancora applicabili e portino i risultati desiderati."},"IntroduzioBigData/Machine-Learning/Machine-Learning":{"slug":"IntroduzioBigData/Machine-Learning/Machine-Learning","filePath":"IntroduzioBigData/Machine Learning/Machine Learning.md","title":"Machine Learning","links":["IntroduzioBigData/Machine-Learning/ML-Supervisionato","IntroduzioBigData/Machine-Learning/ML-NON-Supervisionato/ML-Non-Supervisionato","IntroduzioBigData/Machine-Learning/ML-NON-Supervisionato/PCA","Deep-Learning"],"tags":[],"content":"il ML è l’integrazione di differenti tecniche allo scopo di estrarre valore dai dati.\nML Supervisionato\n-Dato un insieme di dati si vuole andare a scegliere la funzione che meglio approssima l’insieme dei dati considerato, avendo però come input la tipologia di funzione e la classe di funzione che si va a considerare.\nML Non Supervisionato\nDato l’insieme dei dati xi, con i= 1, … , n si vuole inferire la struttura intrinseca della matrice o del vettore X. Questa tecnica è utilizzata tipicamente per fare analisi esplorativa dei dati, poiché non si attribuisce alcuna classificazione, piuttosto che nessun valore atteso Y su cui costruire la funzione f (X); semplicemente si caratterizza in gruppi omogenei il dataset di input.Il ML Non Supervisionato presenta dei limiti: oggettivamente non ha uno scopo singolo e specifico e non si conosce a priori il numero di cluster in cui dividere il dataset (si decide soggettivamente in quanti cluster suddivider il dataset); inoltre non c’è modo di fare verifiche incrociate (Nel ML Supervisionato si divideva in dataset in un training dataset e in un test dataset, e si verificava che il modello applicato al training dataset fosse valido per il test dataset. Nel caso ML Non Supervisionato questo non è più valido: dato un dataset si identificano i cluster) di cui si compone, ma se cambia il dataset cambiano anche i cluster.)\nclustering\n*PCA\nDeep Learning"},"IntroduzioBigData/Machine-Learning/Statistica/Distanza":{"slug":"IntroduzioBigData/Machine-Learning/Statistica/Distanza","filePath":"IntroduzioBigData/Machine Learning/Statistica/Distanza.md","title":"Distanza","links":[],"tags":[],"content":"L’accezione matematica del termine distanza ha un significato analogo a quello dell’uso comune, cioè quello della misura della “lontananza” tra due punti di un insieme al quale si possa attribuire qualche carattere spaziale.\nPiù in generale nello spazio euclideo si può definire la distanza tra due punti {\\displaystyle (x_{1},x_{2},\\ldots ,x_{n})} e  {\\displaystyle (y_{1},y_{2},\\ldots ,y_{n})} nei seguenti modi:\n1-distanza ={\\sum _{i=1}^{n}|x_{i}-y_{i}|} (Distanza di Manhattan)\ndistanza euclidea 2-distanza = {\\sqrt {\\sum _{i=1}^{n}|x_{i}-y_{i}|^{2}}}\n∞ -distanza = {\\lim _{p\\to \\infty }{\\sqrt[{p}]{\\sum _{i=1}^{n}|x_{i}-y_{i}|^{p}}}=\\max _{i=1,\\cdots ,n}\\{|x_{i}-y_{i}|\\}}\nLa 2-distanza in uno spazio a n dimensioni corrisponde al teorema di Pitagora applicato n-1 volte: è la distanza di uno spazio euclideo, normalmente usata nel piano o nello spazio e viene detta anche distanza pitagorica. La 1-distanza, detta anche distanza L1 o distanza Manhattan, genera invece una geometria diversa, detta geometria del taxi. La ∞-distanza (o distanza L∞) è la cosiddetta distanza di Chebyshev."},"IntroduzioBigData/Machine-Learning/Statistica/Standardizazione":{"slug":"IntroduzioBigData/Machine-Learning/Statistica/Standardizazione","filePath":"IntroduzioBigData/Machine Learning/Statistica/Standardizazione.md","title":"Standardizazione","links":["deviazione-standard","IntroduzioBigData/Machine-Learning/Statistica/Varianza"],"tags":[],"content":"Il Mean Variance Scaling, o Standardizzazione dei dati, è una tecnica di normalizzazione delle feature che consiste nel trasformare le feature in modo che abbiano una media zero e una deviazione standard unitaria. In pratica, per ogni feature, si sottrae la media dei suoi valori e poi si divide per la deviazione standard. Questo processo fa sì che le feature abbiano una distribuzione con media zero e varianza unitaria, rendendo più facile per i modelli di machine learning confrontare e interpretare i dati. Questa tipologia di trasformazione non conserva la sparsità dei dati: i valori che in precedenza erano nulli ora sono modificati. Inoltre, poiché sia la media che la varianza dipendono da tutti i campioni e non solo dal più grande o dal più piccolo, questo metodo, a differenza dei precedenti, non è influenzato dagli outlier.\ni valori della distribuzione normale sono tabulati per media zero e varianza unitaria.\nIl procedimento prevede di sottrarre alla variabile aleatoria la sua media e dividere il tutto per la deviazione standard (cioè la radice quadrata della Varianza):\n${\\displaystyle Z={\\frac {X-\\mu }{\\sigma }}.}$\n"},"IntroduzioBigData/Machine-Learning/Statistica/Varianza":{"slug":"IntroduzioBigData/Machine-Learning/Statistica/Varianza","filePath":"IntroduzioBigData/Machine Learning/Statistica/Varianza.md","title":"Varianza","links":["deviazione-standard","IntroduzioBigData/Machine-Learning/Statistica/Varianza"],"tags":[],"content":"la varianza  (deviazione standard) fornisce una misura della variabilità dei valori assunti dalla variabile stessa di quanto essi si discostino quadraticamente rispettivamente dalla media aritmetica. La varianza è una misura di dispersione, ossia una misura di quanto un dato insieme di numeri si discosta dal suo valore medio. Se assume valori lontani dalla sua media, la sua varianza sarà conseguentemente grande. Al contrario, se la variabile aleatoria rimane costante la sua variante sarà nulla\n{\\displaystyle \\sigma _{X}^{2}={\\frac {\\sum _{i}(x_{i}-\\mu _{X})^{2}}{n}}}\ndove  {\\displaystyle \\textstyle \\mu _{X}={\\frac {\\sum _{i}x_{i}}{n}}} di X\nLa formula può essere espressa anche dal punto vista della probabilità, la varianza della variabile aleatoria X  è definita come il valore atteso del quadrato della variabile aleatoria centrata\n{\\displaystyle \\sigma _{X}^{2}=\\mathbb {E} {\\Big [}{\\big (}X-\\mathbb {E} [X]{\\big )}^{2}{\\Big ]}.}\nLo scarto quadratico medio o deviazione standard  (σ)  è la radice quadrata della Varianza ( la quale viene coerentemente rappresentata con il quadrato di sigma,  {\\displaystyle \\sigma ^{2}}.)"},"IntroduzioBigData/Machine-Learning/Statistica/covarianza":{"slug":"IntroduzioBigData/Machine-Learning/Statistica/covarianza","filePath":"IntroduzioBigData/Machine Learning/Statistica/covarianza.md","title":"covarianza","links":[],"tags":[],"content":"la covarianza di due variabili  {\\displaystyle \\textstyle \\sigma _{X,Y}={\\text{Cov}}(X,Y)} , è un indice di variabilità congiunta su una popolazione di N osservazioni congiunte  {\\displaystyle (x_{i},y_{i})} , di rispettive  medie {\\displaystyle {\\bar {x}}} e  {\\displaystyle {\\bar {y}}} la covarianza osservata è:\n{\\displaystyle \\sigma _{X,Y}={\\frac {1}{N}}\\sum _{i=1}^{N}(x_{i}-{\\bar {x}})(y_{i}-{\\bar {y}})={\\frac {1}{N}}\\sum _{i=1}^{N}x_{i}y_{i}-\\left({\\frac {1}{N}}\\sum _{i=1}^{N}x_{i}\\right)\\left({\\frac {1}{N}}\\sum _{i=1}^{N}y_{i}\\right).}"},"IntroduzioBigData/Machine-Learning/Statistica/matrice-di-covarianza":{"slug":"IntroduzioBigData/Machine-Learning/Statistica/matrice-di-covarianza","filePath":"IntroduzioBigData/Machine Learning/Statistica/matrice di covarianza.md","title":"matrice di covarianza","links":["IntroduzioBigData/Machine-Learning/Statistica/matrice-di-covarianza","IntroduzioBigData/Machine-Learning/Statistica/covarianza"],"tags":[],"content":"Calcolo della matrice di covarianza di X: S\nLa matrice di covarianza è una matrice simmetrica p × p (dove p è il numero di dimensioni) che ha come voci le covarianze associate a tutte le possibili coppie delle variabili iniziali. Ad esempio, per un set di dati tridimensionale con 3 variabili x , y e z , la matrice di covarianza è una matrice 3 × 3 di questo da:\n\nPoiché la covarianza di una variabile con se stessa è la sua varianza (Cov(a,a)=Var(a)), nella diagonale principale (in alto da sinistra in basso a destra) abbiamo effettivamente le varianze di ciascuna variabile iniziale. E poiché la covarianza è commutativa (Cov(a,b)=Cov(b,a)), le voci della matrice di covarianza sono simmetriche rispetto alla diagonale principale, il che significa che le porzioni triangolari superiore e inferiore sono uguali.\nCosa ci dicono le covarianze che abbiamo come voci della matrice sulle correlazioni tra le variabili?\nIn realtà è il segno della covarianza che conta:\n\nse positivo allora : le due variabili aumentano o diminuiscono insieme (correlate)\nse negativo allora: uno aumenta quando l’altro diminuisce (correlato inversamente)\nOra che sappiamo che la matrice di covarianza non è altro che una tabella che riassume le correlazioni tra tutte le possibili coppie di variabili, passiamo al passaggio\n"},"IntroduzioBigData/NoSql-Databases/Funzione-hash":{"slug":"IntroduzioBigData/NoSql-Databases/Funzione-hash","filePath":"IntroduzioBigData/NoSql Databases/Funzione hash.md","title":"Funzione hash","links":[],"tags":[],"content":"Cos’è una Funzione Hash?\nUna funzione hash è un algoritmo matematico che prende un input (spesso chiamato “chiave” o “messaggio”) di dimensione arbitraria e restituisce un output di dimensione fissa, solitamente un numero intero, chiamato valore hash, codice hash, digest hash o semplicemente hash. Immagina di avere una macchina che, non importa cosa tu le dia in pasto (una parola, una frase, un intero libro), sputa sempre fuori una stringa di caratteri di una lunghezza predefinita.\nCaratteristiche Fondamentali delle Funzioni Hash\nPer essere efficace in una tabella hash o in altre applicazioni, una funzione hash dovrebbe avere alcune proprietà chiave:\n1. Deterministica:\nPer lo stesso input, la funzione deve sempre produrre lo stesso output. Se dai “ciao” alla funzione oggi e ottieni “123”, anche domani, dandole “ciao”, dovrai ottenere “123”.\n2. Veloce da Calcolare:\nIl calcolo dell’hash deve essere rapido ed efficiente per non rallentare le operazioni della struttura dati.\n3.Distribuzione Uniforme (ideale):\nL’obiettivo è che la funzione distribuisca gli input in modo più uniforme possibile tra tutti i possibili valori hash. Questo significa che dovrebbe minimizzare le collisioni, cioè situazioni in cui due input diversi producono lo stesso valore hash. Se la funzione raggruppa molti input sullo stesso hash, la tabella hash perderà efficienza.\n4. Resistenza alle Collisioni (per funzioni crittografiche):\nNelle funzioni hash crittografiche (come MD5, SHA-256), questa proprietà è cruciale. Significa che deve essere estremamente difficile trovare due input diversi che producono lo stesso output hash. Questo le rende utili per verificare l’integrità dei dati.\nTipi di Funzioni Hash e Loro Applicazioni\nEsistono vari tipi di funzioni hash, progettate per scopi diversi:\n\nFunzioni Hash per Tabelle Hash (Non Crittografiche):\n\nUtilizzate principalmente per le tabelle hash per mappare in modo efficiente le chiavi a posizioni di memoria.\nL’obiettivo principale è la velocità e la buona distribuzione per minimizzare le collisioni.\nEsempi: Spesso usano operazioni matematiche come bitwise shifts, XOR, moltiplicazioni e divisioni (modulo) per mescolare i bit dell’input e produrre un h\n\n\nFunzioni Hash Crittografiche:\n\nMolto più complesse e robuste.\nDevono essere unidirezionali (impossibile risalire all’input partendo dall’hash) e avere una forte resistenza alle collisioni.\nUtilizzi:\n\nVerifica dell’integrità dei dati: Se anche un singolo bit di un file cambia, il suo hash cambia completamente. Questo permette di rilevare manomissioni.\nPassword: Anziché memorizzare le password in chiaro, i sistemi memorizzano il loro hash. Quando un utente tenta di accedere, la password inserita viene hashata e confrontata con l’hash memorizzato.\nFirme digitali: Per garantire l’autenticità e l’integrità dei documenti.\nBlockchain: Fondamentali per la sicurezza e l’immutabilità della catena.\n\n\nEsempi noti: MD5 (obsoleto per sicurezza ma ancora usato in alcuni contesti non critici), SHA-1 (anche questo obsoleto per sicurezza), SHA-256, SHA-512 (famiglia SHA-2), e più recenti come SHA-3.\n\n\n\nL’Importanza della Funzione Hash\nLa qualità di una funzione hash è cruciale per le prestazioni e la sicurezza delle strutture dati e dei sistemi che la utilizzano. Una funzione hash ben progettata garantisce rapidità nelle operazioni di ricerca e inserimento, mentre una funzione hash crittografica robusta è alla base della sicurezza di molte tecnologie digitali che usiamo ogni giorno."},"IntroduzioBigData/NoSql-Databases/Key-Value":{"slug":"IntroduzioBigData/NoSql-Databases/Key-Value","filePath":"IntroduzioBigData/NoSql Databases/Key-Value.md","title":"Key-Value","links":["IntroduzioBigData/NoSql-Databases/Tabella-hash","funzione-hash"],"tags":[],"content":"I database Key-Value sono basati sul concetto di Associative Arrays(è un array i cui elementi sono accessibili mediante stringhe anziché indici puramente numerici), ottenuti tramite l’associazione tra chiave e valore/i. Questi sono un concetto molto più flessibile da un lato e molto più ampio dall’altro dei vettori classici: non sono ristretti come i normali array, e permettono di utilizzare interi come indici o di limitare i valori allo stesso tipo.\nUn database Key-Value altro non è che una Tabella hash che rappresenta un array associativo, le cui chiavi sono uniche, quindi gli indici sono unici, e possono essere di qualsiasi tipo.\nLa chiave viene trasformata in una chiave hash attraverso l’utilizzo di una funzione hash:\n\nsi parte dalla stringa che rappresenta la chiave e la si converte in una stringa di interi (valore binari), costituita da un Major Key Components (MKC),\ne opzionalmente, da uno o più minor key components (mkc).\nSe i mkc sono utilizzati, la combinazione del MKC e del/dei mkc identifica univocamente un singolo record nello store. Questo è fatto in maniera tale che le chiavi vengano ad essere distribuite uniformemente nel database distribuito lungo le partizioni orizzontali attraverso l’utilizzo della MKC, che identifica in quale Shard verrà ad essere memorizzato un dato record.\nLe Shards sono una partizione orizzontale dei dati nel database. Ogni Shard è gestita da un determinato Database Server ed è identificata da una MKC. Questa è una modalità di partizionamento cosiddetta orizzontale, che è diversa da quella classica dei database relazionali (che è una partizione verticale), in cui i dati sono partizionati su server diversi ma per colonna\nAltro concetto importante riguarda i valori e la loro gestione nei database distribuiti NoSQL. I valori non sono altro che arrays di byte che non richiedono strong typing: possono contenere stringhe o insiemi di stringhe con l’attributo che le contraddistingue.\n\n\n\n                  \n                  Schema Avro \n                  \n                \n\n\nIn un caso del genere conviene quindi utilizzare il concetto di Schema Avro (basato sul formato JSON) che permette semplicemente di andare a definire degli schemi all’interno del valore, quindi permettendo ai database Key-Value di tipizzare il contenuto del valore. Tipicamente lo Schema Avro è salvato all’interno di un file che lo descrive. Per poterlo utilizzare si fa un binding: attraverso il comando ddl add-schema (kv → ddl add-schema -file PersonSchema.avsc) si aggiunge al Key-Value che sto considerando lo Schema Avro in modo tale che possa leggerlo. Per rendere disponibile lo Schema Avro al codice bisogna effettuare un parsing (final Schema.Parser parse = new Schema.Parser();), e andare a leggerlo (perser.parse(new File(“PersonSchema.asvc”))), quin- di renderlo disponibile all’applicazione (final Schema PersonSchema = parser.getTypes().get (“FVavro.PersonaInformation”)). Questo consentirà di andare a leggere il contenuto dello schema stesso in termini di tipi resi disponibili, quindi sarà possibile aggiungere i campi che contraddistinguono lo schema dell’applicazione, creando un binding, quindi una translation dal valore contenuto nel database Key- Value alle istanze di Schema Avro che si vanno a considerare. Solo a questo punto si hanno a disposizione i contenuti singoli di ciascun campo contenuto all’interno dello Schema Avro definito per quel particolare value associato alla chiave. Questo è valido per qualsiasi operazione si effettui su un database key-value store. Tali operazioni posso essere quindi riassunte nelle seguenti: 1. Retrieve di un valore attraverso una singola chiave; 2. Inserire un valore, o modificarlo, attraverso la chiave; 3. Cancellare il valore attraverso la chiave. In alcuni database key-value si possono utilizzare le versioni da associare al valore per migliorare la consistenza. Quindi in generale, viste le operazioni che è possibile effettuare su un Key-Value database, si possono avere difficoltà quando si vanno ad effettuare operazioni di ricerca.\n\n\n\nkey-value architecture\nL’architettura di un database Key-Value è costituita da:\n\nStorage Nodes ⇒  ciascun Storage Node contiene uno o più Replication Nodes a seconda della sua capacità in termini di storage\n\nReplication Nodes ⇒ ogni Replication Node si può pensare come a un singolo database contenente tabelle o coppie chiave-valore suddivise in almeno una partizione:\n\nPartizione ⇒ Una partizione è un sottoinsieme dei dati totali, raggruppato in base a una chiave hash. Quindi le chiavi sono inserite in contenitori logici chiamati partizioni\nI Replication Nodes sono organizzati in Shard . Un singolo Shard contiene più nodi di replicazione e un nodo master . Il nodo master esegue tutte le attività di scrittura sul database. Ogni Shard contiene anche una o più repliche di sola lettura . Il nodo master copia tutti i nuovi dati delle attività di scrittura sulle repliche. Le repliche vengono quindi utilizzate per gestire le operazioni di sola lettura.\nIn caso di guasto della macchina che ospita il nodo master, il master esegue automaticamente il failover su uno degli altri nodi nello shard. Uno dei nodi replica viene promosso automaticamente a master. Sebbene possa esserci un solo nodo master per shard in un dato momento, qualsiasi altro membro dello shard può diventare un nodo master.\nQuindi lo Shard è composto da:\nUno Storage Node “Replication Master” (o Leader): Questo è il nodo principale responsabile delle operazioni di scrittura per e coordina le repliche.\nUno o più Storage Node “di Replica” (o Follower): Questi sono nodi aggiuntivi che contengono copie identiche (repliche) dei dati del nodo primario. Il loro scopo è garantire la disponibilità dei dati in caso di fallimento del nodo primario e, spesso, possono anche servire le letture.\nLa porzione logica dei dati o partizioni: Questa è la parte specifica del dataset totale che questo gruppo di nodi è responsabile di gestire.\n\n\n\n\n\n\nIn generale un’architettura tipica utilizzata da un’applicazione che utilizza un database NoSQL .\n"},"IntroduzioBigData/NoSql-Databases/NoSql-Database":{"slug":"IntroduzioBigData/NoSql-Databases/NoSql-Database","filePath":"IntroduzioBigData/NoSql Databases/NoSql Database.md","title":"NoSql Database","links":["IntroduzioBigData/NoSql-Databases/Key-Value"],"tags":[],"content":"Key-value, Documentali, Colonnari, Grafo\nAlla fine degli anni 90’ e nei primi anni 2000, quando sono iniziate a nascere le applicazioni web caratte rizzate da una impredicibilità dei dati da gestire (in termini di volume e varietà) e del numero degli utenti, i progettisti hanno iniziato a riscontrare diverse tipologie di problemi legati ai tradizionali Relational Database Management System (RDBMS), in termini di Scalabilità, Costi, Flessibilità e Disponibilità.\nTutto questo ha fatto si che si pensassero nuove architetture basate su sistemi distribuiti, e caratterizzate dalla capacità di gestire grandi volumi di dati secondo concetti più semplici che indirizzano questi problemi. Esistono diverse tipologie di database NoSQL, tra cui:\n\nKey-Value (Redis, DynamoDB): database che gestiscono oggetti formati da coppie di chiave (identificativo del record) e valore (possono esserci anche più valori), che possono anche essere di diverso tipo;\nDocumentali (MongoDB, CouchDB): estensione dei Key-Value, associano ad una chiave un documento, quindi un testo in formato JSON o XML che può essere ricorsivo, quindi può contenere tipologie di dati molto variegati;\nColonnari(Cassandra, HBase): database che lavorano su base colonna, dove un record (unità informativa) può essere costituito da colonne diverse da quelle degli altri record (non vi è un numero fisso di colonne e un numero variabile di record; vi è un numero variabile di righe e di colonne), aumentando la flessibilità del sistema stesso;\nA grafo (Neo4j): i dati sono gestiti tramite nodi (i dati stessi) e archi (relazioni tra i dati). Tali database sono fatti per gestire e andare ad interrogare relazioni e percorsi possibili tra i nodi, specie in problemi di analisi predittiva, in cui si vogliono scoprire potenziali relazioni di causa-effetto tra nodi.\n\nObiettivi: Persistenza, consistenza, disponibilità\nGli obiettivi di un qualsiasi sistema di gestione dei dati sono:\n• garantire la persistenza dei dati: nel caso di failure del sistema di gestione del database, i dati in esso contenuti devono essere comunque disponibili non appena ripristinata la situazione corretta, quindi non devono andar persi;\n• mantenere la consistenza dei dati: a fronte di una operazione di scrittura o di aggiornamento del contenuto, i dati possono essere letti in modo consistente rispetto alle operazioni effettuate;\n• assicurare la disponibilità dei dati: a fronte di guasti di uno degli item dell’infrastruttura, è necessario mantenere la disponibilità di accesso in lettura o scrittura ai dati stessi.\nNel caso di sistemi distribuiti la consistenza e la disponibilità generano delle situazioni di difficoltà, specie per le performance. In termini di gestione della consistenza, quando effettuano delle operazioni di scrittura, il fatto di generare repliche può non essere un problema, a che si verifichi un guasto sul primo server prima che vada completata l’operazione di scrittura prima replica; in questo caso si avrebbe una potenziale inconsistenza dei dati. Nel caso di operazioni di lettura invece è difficile capire se si è in una situazione di inconsistenza del dato, non avendo modo di osservare se magari nel frattempo sono state effettuate richieste di scrittura e/o aggiornamento. Dunque il concetto che ne nasce è quello di Eventual Consistency.\nEventual consistency\novvero la possibilità per un database di consentire operazioni di modifica, e di ottenere la consistenza generale del sistema solo dopo il verificarsi di un certo evento, al termine di tutte le operazioni. Ne deriva un ulteriore concetto, quello di Quorum, ossia il numero di server che devono rispondere ad operazioni di lettura e/o scrittura affinché esse siano considerate complete. Potrebbe quindi essere necessario definire il Quorum in maniera corretta, come: Il corretto bilanciamento tra le necessità di Consistenza rispetto ai Tempi di Risposta e rispetto alla Durabilità, ossia la proprietà di mantenere la versione corretta dei dati per un lungo periodo di tempo. Questo significa che la Consistenza e il Tempo di Risposta sono direttamente correlate.\n\n\n                  \n                  Esempio Quorum \n                  \n                \n\n\nNel caso di Quorum pari a 3 si avrebbe il tempo di risposta necessario ad effettuare l’aggiornamento su 3 server; se il Quorum fosse 1, il tempo di risposta sarebbe minore, ma non si avrebbe sufficiente e necessaria Consistenza, oltre ad una scarsa Durabilità nel caso in cui il server fallisse.\n\n\n\nAltro concetto fondamentale per i database distribuiti è il cosiddetto Teorema CAP, noto anche come Teorema di Brewer. Questo afferma che i database distribuiti non possono fornire al tempo stesso Consistency, Availability (intesa come la capacità di rispondere a una qualsiasi query in un tempo ristretto) e Partition Protection(intesa come la capacità dei server del database di continuare ad essere disponibili e a fornire i dati anche in seguito ad un fallimento di una partizione della rete, ripristinando la situazione precedente). Quindi bisogna lavorare per far si che i database distribuiti abbiano il giusto bilanciamento tra queste tre caratteristiche. Ancora a differenza dei tradizionali database relazionali, che godono delle proprietà Atomicity, Consistency, Isolation, Durability (ACID), i database NoSQL godono delle proprietà Basically Available, Soft state, Eventually Consistent (BASE): disponibilità di tipo basico (non totale), non garantiscono che i dati siano sempre in uno stato univoco fin quando l’operazione non si è propagata su tutto il database distribuito da considerarsi consistente, e sono Eventual Consistency. Esistono diverse tipologie di Eventual Consistency: • Causal Consistency, deve essere mantenuto il rapporto di causa-effetto tra le operazioni quando si effettuano una sequenza di operazioni; • Read-Your-Write Consistency, quando un utente effettua una scrittura, poi un’operazione di lettura, que- sta ritorna esattamente quello che era stato lanciato in scrittura poco prima, garantendo la consistenza dell’operazione di scrittura/lettura; • Session Consistency, all’interno della sessione utente le operazioni effettuate (lettura, scrittura, modifica) sono consistenti tra loro e seguono il filone applicativo che le distingue; • Monotonic Read Consistency, se vi è una sequenza di operazioni di lettura fermo restando il contenuto del database, ottengo sempre il medesimo set di risultati; • Monotonic Write Consistency, se vi è una sequenza di operazioni di scrittura e/o aggiornamento, ottengo a fronte della medesima sequenza di operazioni, il medesimo risultato finale."},"IntroduzioBigData/NoSql-Databases/Tabella-hash":{"slug":"IntroduzioBigData/NoSql-Databases/Tabella-hash","filePath":"IntroduzioBigData/NoSql Databases/Tabella hash.md","title":"Tabella hash","links":["IntroduzioBigData/NoSql-Databases/Funzione-hash","Funzione-Hash"],"tags":[],"content":"Una tabella hash (o in inglese hash table) è una struttura dati utilizzata per archiviare e recuperare dati in modo molto efficiente. Funziona mappando le chiavi a valori, in modo simile a un dizionario, ma con un meccanismo interno che la rende estremamente veloce per le operazioni di ricerca, inserimento e cancellazione.\nL’idea fondamentale di una tabella hash è quella di utilizzare una Funzione hash per calcolare una posizione (un “indice” o “indirizzo”) all’interno di un array (spesso chiamato “array di slot” o “bucket”) dove il dato deve essere memorizzato.\nCome funziona una Tabella Hash:\n\nChiave (Key): È il dato che vuoi utilizzare per identificare univocamente un valore. Ad esempio, in una rubrica, il nome di una persona potrebbe essere la chiave.\nValore (Value): È il dato che vuoi archiviare, associato a una chiave. Ad esempio, il numero di telefono di quella persona.\nFunzione Hash: È un algoritmo matematico che prende la chiave in input e restituisce un numero intero (l’indice) che corrisponde a una posizione nell’array. L’obiettivo della funzione hash è quello di distribuire le chiavi in modo uniforme il più possibile nell’array per minimizzare le collisioni.\nArray (o Tabella): È l’area di memoria dove i dati vengono effettivamente memorizzati, in base agli indici calcolati dalla funzione hash.\n\nProcesso di Inserimento:\n\nPrendi la chiave del dato da inserire.\nApplica la funzione hash alla chiave per ottenere un indice numerico.\nMemorizza il valore associato alla chiave nella posizione dell’adicata da quell’indice.\nProcesso di Ricerca:\nPrendi la chiave del dato che vuoi cercare.\nApplica la stessa funzione hash alla chiave per ottenere l’indice.\nVai alla posizione nell’array indicata da quell’indice e recupera il valore.\n\nVantaggi delle Tabelle Hash:\n\nVelocità: In media, le operazioni di ricerca, inserimento e cancellazione hanno una complessità temporale di O(1) (tempo costante). Questo significa che il tempo necessario per queste operazioni rimane praticamente lo stesso indipendentemente dalla quantità di dati, purché la tabella sia ben progettata e le collisioni siano gestite efficacemente.\nEfficienza: Sono molto efficienti per i casi d’uso in cui è necessario accedere rapidamente ai dati tramite una chiave.\n\nSvantaggi e Problemi:\n\nCollisioni: È il problema principale. Due chiavi diverse possono produrre lo stesso indice tramite la funzione hash. Quando questo accade, si verifica una collisione. Le collisioni devono essere gestite per evitare la perdita di dati o l’errata lettura.\n\nGestione delle Collisioni: Le tecniche più comuni includono:\n\nConcatenamento (Chaining): Ogni slot dell’array non contiene un singolo dato, ma una lista (o un’altra struttura dati) di tutti i dati che hanno lo stesso hash.\nIndirizzamento Aperto (Open Addressing): Se uno slot è già occupato, si cerca sistematicamente un altro slot libero (es. linear probing, quadratic probing, double hashing).\n\n\n\n\nCosto della funzione hash: Una funzione hash mal progettata o troppo complessa può rallentare le operazioni.\nFattore di Carico (Load Factor): È il rapporto tra il numero di elementi memorizzati e la dimensione totale dell’array. Un fattore di carico troppo alto (tabella troppo piena) aumenta la probabilità di collisioni e degrada le prestazioni. Spesso, quando il fattore di carico supera una certa soglia, la tabella viene “ridimensionata” (rehashed), cioè si crea un array più grande e tutti gli elementi vengono ri-inseriti con la funzione hash.\n\nApplicazioni Comuni:\nLe tabelle hash sono onnipresenti nell’informatica e vengono usate in:\n\nDatabase: Per indicizzare i dati e velocizzare le query.\nCache: Per memorizzare dati a cui si accede frequentemente.\nCompilatori: Per tabelle dei simboli.\nSistemi operativi: Per gestire le tabelle dei processi o dei file.\nLinguaggi di programmazione: Molti linguaggi (come Python con i suoi dizionari, Java con le HashMap, JavaScript con gli oggetti) implementano strutture dati basate sulle tabelle hash.\nCrittografia: Le funzioni hash crittografiche sono un tipo specifico di funzione hash utilizzata per garantire l’integrità dei dati e per la sicurezza.\nIn sintesi, una tabella hash è uno strumento potentissimo per l’archiviazione e il recupero efficiente dei dati, purché le collisioni siano gestite in modo appropriato e la funzione hash sia ben scelta.\n"},"IntroduzioBigData/NoSql-Databases/Teorema-Cap":{"slug":"IntroduzioBigData/NoSql-Databases/Teorema-Cap","filePath":"IntroduzioBigData/NoSql Databases/Teorema Cap.md","title":"Teorema Cap","links":[],"tags":[],"content":"afferma che è impossibile per un sistema informatico distribuito fornire simultaneamente tutte e tre le seguenti garanzie: completa coerenza dei dati, continua disponibilità e tolleranza alle partizioni, quindi è necessario stabilire, di volta in volta, quali di queste tre garanzie sacrificare.\nCoerenza (Consistency)\nUn sistema è definito completamente coerente quando è in grado di garantire che una volta memorizzato un nuovo stato nel sistema, questo è utilizzato in ogni operazione successiva fino alla successiva modifica dello stesso. Pertanto, tutte le richieste dello stato del sistema, nell’arco di tempo che intercorre tra uno stato e quello successivo, forniscono il medesimo risultato. Per esempio, quando si ha a che fare con una cache con un singolo nodo, a meno di errori di codifica, la cache è intrinsecamente totalmente coerente poiche‘ lo stato è aggiornato in un nodo e mantenuto esclusivamente nello stesso. Un singolo nodo quindi garantisce intrinsecamente la totale consistenza e anche la tolleranza alle partizioni, ma non una sufficiente disponibilità (e tolleranza ai guasti) e, in scenari non banali, tende a presentare scarse performance. Quando invece la cache è distribuita e vi sono due o più nodi, il sistema è pienamente consistente quando tutti i nodi sono in grado di lavorare sullo stesso stato, ossia vedono gli stessi dati con gli stessi valori.. Sebbene non sia possibile garantire la terna CAP, è necessario implementare strategie per ridurre l’impatto dovuto al manifestarsi della garanzia trascurata (per esempio perdita di coerenza o generazione di una partizione).\nAvabilty (disponibilità)\nUn sistema è detto continuamente disponibile quando è sempre in grado di soddisfare le varie richieste/erogare i propri servizi.\nQualora il sistema distribuisce le informazioni su più nodi in modo ridondante. Chiaramente con l’aumento del numero dei nodi aumenta anche la disponibilità del sistema.\nDa tener presente che il traffico di rete tra i nodi, necessario per garantire la consistenza, è proporzionale al loro numero degli stessi. Quindi aumentando il numero di nodi oltre un certo livello di soglia si assistere a un degradamento delle performance. Inoltre, aumentando i nodi aumentano i problemi relativi alla garanzia della tolleranza alle partizioni.\nPartition tollerance\nla tolleranza alle partizioni come la proprietà di un sistema di continuare a funzionare correttamente anche in presenza di una serie di fallimenti dell’infrastruttura fino a che l’interno network fallisca.\nPer dimostrare questa tolleranza alle partizioni si consideri una configurazione (da ponderare molto attentamente nel contesto delle cache distribuite) in cui un singolo cluster preveda nodi presenti su diversi data center. Qualora, per qualsiasi motivo si dovesse perdere la connettività di rete tra i vari data center, oppure le performance di rete degradino improvvisamente per un arco di tempo prolungato, si genera la situazione in cui i server del cluster non sono più in grado di sincronizzare lo stato del sistema. In queste circostanze, i server in ogni centro si riorganizzano in sotto-cluster assumendo che quelli dell’altro centro non siano più raggiungibili, tagliandoli fuori. In questo modo si finisce per generare due sotto-cluster che danno vita al classico scenario denominato del “cervello diviso“. Il sistema continua a funzionare, i dati sono gestiti nei sotto-cluster in maniera non coordinata causando tutta una serie di problemi inclusi la perdita di dati. Come caso semplice si consideri un sistema di prenotazione. In un contesto del genere è assolutamente possibile che ogni cluster finisca per assegnare la medesima prenotazione a diversi clienti."},"index":{"slug":"index","filePath":"index.md","title":"Homepage","links":["IntroduzioBigData/nota1","IntroduzioBigData/nota2"],"tags":[],"content":"Benvenuto nel sito.\nEcco le mie note:\n\nIntroduzione ai Big Data\nAltra nota\n"}}